" lots; sum of unique block-group pop_tot = ", total_pop)
# 0) Install & load packages (run install.packages() once, then comment it out)
install.packages(c("vroom", "dplyr", "stringr", "lubridate", "readr"))
library(vroom)
library(dplyr)
library(stringr)
library(lubridate)
library(readr)
# 1) Path to raw 311 data
data_path <- "data/raw/311_Service_Requests_from_2010_to_Present_20250526.csv"
# 2) Preview 5 000 rows to confirm columns
rats_5k_sample <- vroom(
file      = data_path,
n_max     = 5000,
col_select = c(
"Unique Key", "Created Date", "Closed Date", "Status",
"Resolution Description", "Location Type",
"Incident Zip", "Borough", "BBL",
"Latitude", "Longitude", "Descriptor"
)
)
# Inspect
print(head(rats_5k_sample, 5))
print(names(rats_5k_sample))
# 3) Parse dates, flag bad closes (pre-2010), compute days_open as of 2025-05-26,
#    and also flag stale_open if days_open > 365
# a) Define your fixed sentinel & snapshot dates
sentinel_date <- as_datetime("2010-01-01 00:00:00")  # anything before this is bogus
snapshot_date <- as.Date("2025-05-26")               # cut-off for still-open
rats_clean <- vroom(
file       = data_path,
col_select = c(
"Unique Key", "Created Date", "Closed Date", "Status",
"Resolution Description", "Location Type",
"Incident Zip", "Borough", "BBL",
"Latitude", "Longitude", "Descriptor"
),
col_types = cols(
`Unique Key`             = col_character(),
`Created Date`           = col_character(),
`Closed Date`            = col_character(),
Status                   = col_character(),
`Resolution Description` = col_character(),
`Location Type`          = col_character(),
`Incident Zip`           = col_character(),
Borough                  = col_character(),
BBL                      = col_character(),
Latitude                 = col_double(),
Longitude                = col_double(),
Descriptor               = col_character()
)
) %>%
mutate(
# parse raw strings
created_dt    = mdy_hms(`Created Date`),
closed_dt_raw = mdy_hms(`Closed Date`),
# flag any closed before sentinel_date
bad_close     = closed_dt_raw < sentinel_date,
# recode closed_dt: NA if bad_close or truly missing
closed_dt     = if_else(bad_close, NA_POSIXct_, closed_dt_raw),
# compute days_open: real closes vs snapshot for pending
days_open     = as.numeric(
if_else(
!is.na(closed_dt),
difftime(closed_dt, created_dt, units = "days"),
difftime(snapshot_date, as.Date(created_dt), units = "days")
),
units = "days"
),
# flag extremely long‚Äêopen tickets (> 365 days)
stale_open    = days_open > 365,
# classify event type for modeling later
event_type = case_when(
str_detect(Descriptor, regex("sighting",   ignore_case = TRUE)) ~ "direct",
str_detect(Descriptor, regex("dropp|burrow",ignore_case = TRUE)) ~ "sign",
TRUE                                                            ~ "other"
)
) %>%
# keep only the fields we need
select(
`Unique Key`, created_dt, closed_dt, days_open,
bad_close, stale_open, event_type,
Status, `Resolution Description`,
`Location Type`, `Incident Zip`, Borough, BBL,
Latitude, Longitude, Descriptor
) %>%
# filter to your rodent-related reports
filter(
str_starts(Descriptor, regex("Rat|Rodent|Mouse",   ignore_case = TRUE)) |
str_detect(Descriptor,   regex("dropp|burrow|bait|nest|runway|gnaw", ignore_case = TRUE))
)
# 4) Final check
message("‚úÖ Clean & flagged: ", nrow(rats_clean), " rows; ",
sum(rats_clean$bad_close), " bad_close, ",
sum(rats_clean$stale_open), " stale_open")
glimpse(rats_clean)
# 5) Export cleaned rats dataset
write_csv(
rats_clean,
"data/processed/rats_clean.csv"
)
message("‚úèÔ∏è Saved rats_clean.csv to data/processed/")
# 0) Install & load needed packages (once; then comment install.packages)
# install.packages(c("tidycensus", "sf", "dplyr", "readr", "stringr"))
library(tidycensus)   # to pull ACS data
library(sf)           # spatial tools for centroids
library(dplyr)        # data wrangling
library(readr)        # CSV I/O
library(stringr)      # string helpers
# 1) Load your Census API key from ~/.Renviron
my_key <- Sys.getenv("CENSUS_API_KEY")
if (my_key == "") stop("üîë CENSUS_API_KEY not found in ~/.Renviron")
census_api_key(my_key, install = FALSE)
# 2) Define ACS variables (2019 5-year)
vars <- c(
pop_tot    = "B01003_001",  # total population
med_income = "B19013_001",  # median household income
pov_count  = "B17001_002"   # count below poverty
)
# 3) Download ACS block-group data (with geometry)
message("üîÑ Downloading ACS block-group data for NYC‚Ä¶")
acs_bg <- get_acs(
geography = "block group",
variables = vars,
year      = 2019,
state     = "NY",
county    = c("Bronx", "Kings", "New York", "Queens", "Richmond"),
output    = "wide",
geometry  = TRUE
)
# 4) Read & project PLUTO parcels
message("üîÑ Reading PLUTO parcels (MapPLUTO.shp)‚Ä¶")
bbl_sf <- st_read("data/raw/MapPLUTO.shp") %>%
st_transform(crs = st_crs(acs_bg))
# 4b) Repair any invalid geometries so centroids will compute
if (!all(st_is_valid(bbl_sf))) {
message("üîß Repairing invalid geometries‚Ä¶")
bbl_sf <- sf::st_make_valid(bbl_sf)
}
# 5) Compute centroids of each lot
message("üîÑ Computing centroids of each lot‚Ä¶")
bbl_centroids <- st_centroid(bbl_sf)
# 6) Spatial‚Äêjoin each lot to its block‚Äêgroup, keep GEOID + estimates
message("üîÑ Joining lots to block groups‚Ä¶")
acs_by_bbl <- st_join(
bbl_centroids,
acs_bg %>% select(GEOID, ends_with("E")),
join = st_within
) %>%
st_drop_geometry() %>%
rename_with(~ str_remove(.x, "E$"), ends_with("E")) %>%
select(
BBL,        # tax-lot ID
GEOID,      # block-group ID
pop_tot,    # total population
med_income, # median household income
pov_count   # count below poverty
)
# 7) Write out the ACS-by-BBL table
message("üîÑ Writing ACS√óBBL table to data/raw/ACS.csv‚Ä¶")
write_csv(acs_by_bbl, "data/raw/ACS.csv")
# 8) Sanity check: sum each block-group‚Äôs pop_tot exactly once
total_pop <- acs_by_bbl %>%
distinct(GEOID, pop_tot) %>%
summarise(total_pop = sum(pop_tot, na.rm = TRUE)) %>%
pull(total_pop)
message("‚úÖ Done! ACS table has ", nrow(acs_by_bbl),
" lots; sum of unique block-group pop_tot = ", total_pop)
# 0) Load required packages
library(sf)      # spatial data handling
library(dplyr)   # data wrangling
library(readr)   # CSV import
# 1) Read in your cleaned rats dataset
#    (make sure this matches where you saved it)
rats_clean <- read_csv("data/processed/rats_clean.csv")
# 2) Convert to sf point geometry
rat_sf <- st_as_sf(
rats_clean,
coords = c("Longitude", "Latitude"),  # x = lon, y = lat
crs    = 4326,                        # WGS84
remove = FALSE                        # keep raw lon/lat cols
)
View(acs_by_bbl)
View(acs_bg)
View(acs_by_bbl)
v19 <- load_variables(2019, "acs5", cache = TRUE)
v19 %>%
filter(str_detect(name, "B17001")) %>%
select(name, label)
View(v19)
View(v19)
write_csv(v19, "data/raw/gpt.csv")
View(v19)
# 1.5) Sanity Check if B17010_002 Exsits because NA result previously
v19 <- load_variables(2019, "acs5", cache = TRUE)
v19 %>%
filter(str_detect(name, "B17010_002")) %>%
select(name, label)
v19 <- load_variables(2019, "acs5", cache = TRUE)
v19 %>%
filter(str_detect(name, "B17010")) %>%
select(name, label)
# 0) Install & load needed packages (once; then comment out install.packages calls)
# install.packages(c("tidycensus", "sf", "dplyr", "readr", "stringr"))
library(tidycensus)   # to pull ACS data
library(sf)           # spatial tools for centroids & validity checks
library(dplyr)        # data wrangling
library(readr)        # CSV I/O
library(stringr)      # string helpers
# 1) Load your Census API key from ~/.Renviron
my_key <- Sys.getenv("CENSUS_API_KEY")
if (my_key == "") stop("üîë CENSUS_API_KEY not found in ~/.Renviron")
census_api_key(my_key, install = FALSE)
# 1.5) Sanity Check if B17010_002 Exsits because NA result previously
v19 <- load_variables(2019, "acs5", cache = TRUE)
v19 %>%
filter(str_detect(name, "B17010")) %>%
select(name, label)
# 2) Define ACS variables (2019 5-year)
vars <- c(
pop_tot    = "B01003_001",  # total population
med_income = "B19013_001",  # median household income
pov_count  = "B17010_002"   # total population below poverty level
)
# 3) Download ACS block-group data (with geometry)
message("üîÑ Downloading ACS block-group data for NYC‚Ä¶")
acs_bg <- get_acs(
geography = "block group",
variables = vars,
year      = 2019,
state     = "NY",
county    = c("Bronx", "Kings", "New York", "Queens", "Richmond"),
output    = "wide",
geometry  = TRUE
)
# 4) Read & project PLUTO parcels (MapPLUTO.shp)
message("üîÑ Reading PLUTO parcels (MapPLUTO.shp)‚Ä¶")
bbl_sf <- st_read("data/raw/MapPLUTO.shp") %>%
st_transform(crs = st_crs(acs_bg))
# 4b) Repair any invalid geometries so centroids will compute
if (!all(st_is_valid(bbl_sf))) {
message("üîß Repairing invalid geometries‚Ä¶")
bbl_sf <- sf::st_make_valid(bbl_sf)
}
# 5) Compute centroids of each lot
message("üîÑ Computing centroids of each lot‚Ä¶")
bbl_centroids <- st_centroid(bbl_sf)
# 6) Spatial‚Äêjoin each lot to its block‚Äêgroup, keep GEOID + ACS estimates
message("üîÑ Joining lots to block groups‚Ä¶")
acs_by_bbl <- st_join(
bbl_centroids,
acs_bg %>% select(GEOID, ends_with("E")),
join = st_within
) %>%
st_drop_geometry() %>%
# rename pop_totE ‚Üí pop_tot, med_incomeE ‚Üí med_income, pov_countE ‚Üí pov_count
rename_with(~ str_remove(.x, "E$"), ends_with("E")) %>%
select(
BBL,        # tax-lot ID
GEOID,      # block-group ID
pop_tot,    # total population
med_income, # median household income
pov_count   # count below poverty level
)
# 7) Write out the ACS√óBBL table
message("üîÑ Writing ACS√óBBL table to data/raw/ACS.csv‚Ä¶")
write_csv(acs_by_bbl, "data/raw/ACS.csv")
# 8) Sanity check: sum each block-group‚Äôs pop_tot exactly once
total_pop <- acs_by_bbl %>%
distinct(GEOID, pop_tot) %>%
summarise(total_pop = sum(pop_tot, na.rm = TRUE)) %>%
pull(total_pop)
message(
"‚úÖ Done! ACS table has ",
nrow(acs_by_bbl),
" lots; sum of unique block-group pop_tot = ",
total_pop
)
View(acs_bg)
View(acs_by_bbl)
# 0) Load required packages
library(sf)      # spatial data handling
library(dplyr)   # data wrangling
library(readr)   # CSV import
# 1) Read in your cleaned rats dataset
#    (make sure this matches where you saved it)
rats_clean <- read_csv("data/processed/rats_clean.csv")
# 2) Convert to sf point geometry
rat_sf <- st_as_sf(
rats_clean,
coords = c("Longitude", "Latitude"),  # x = lon, y = lat
crs    = 4326,                        # WGS84
remove = FALSE                        # keep raw lon/lat cols
)
# 0) Load required packages
library(sf)      # spatial data handling
library(dplyr)   # data wrangling
library(readr)   # CSV import
# 1) Read in your cleaned rats dataset
rats_clean <- read_csv("data/processed/rats_clean.csv")
# 1b) Drop any rows with missing coordinates, since st_as_sf() won‚Äôt allow them
na_count <- rats_clean %>%
filter(is.na(Longitude) | is.na(Latitude)) %>%
nrow()
if (na_count > 0) {
message("‚ö† Dropping ", na_count, " rows with missing Longitude/Latitude")
rats_clean <- rats_clean %>%
filter(!is.na(Longitude), !is.na(Latitude))
}
# 2) Convert to sf point geometry
rat_sf <- st_as_sf(
rats_clean,
coords = c("Longitude", "Latitude"),  # x = lon, y = lat
crs    = 4326,                        # WGS84
remove = FALSE                        # keep raw lon/lat cols
)
# 3) Read NYC tax-lot polygons (BBL shapefile)
bbl_sf <- st_read("data/raw/NYC_BBL_shapefile.shp") %>%
st_transform(crs = st_crs(rat_sf))   # ensure same CRS
# 0) Load required packages
library(sf)      # spatial data handling
library(dplyr)   # data wrangling
library(readr)   # CSV import
# 1) Read in your cleaned rats dataset
rats_clean <- read_csv("data/processed/rats_clean.csv")
# 1b) Drop rows with missing coords
missing_n <- rats_clean %>%
filter(is.na(Longitude) | is.na(Latitude)) %>%
nrow()
if (missing_n > 0) {
message("‚ö† Dropping ", missing_n, " rows with missing Longitude/Latitude")
rats_clean <- rats_clean %>%
filter(!is.na(Longitude), !is.na(Latitude))
}
# 2) Convert to sf point geometry
rat_sf <- st_as_sf(
rats_clean,
coords = c("Longitude", "Latitude"),
crs    = 4326,
remove = FALSE
)
# 3) Read NYC tax-lot polygons (PLUTO shapefile)
bbl_sf <- st_read("data/raw/MapPLUTO.shp") %>%
st_transform(crs = st_crs(rat_sf))
# 4) Spatial-join: stamp each sighting with its BBL
rat_bbl <- st_join(
rat_sf,
bbl_sf %>% select(BBL),
join = st_within,
left = TRUE
)
# 0) Load required packages
library(sf)      # spatial data handling
library(dplyr)   # data wrangling
library(readr)   # CSV import
# 1) Read in your cleaned rats dataset
rats_clean <- read_csv("data/processed/rats_clean.csv")
# 1b) Drop any rows with missing coords
missing_n <- rats_clean %>%
filter(is.na(Longitude) | is.na(Latitude)) %>%
nrow()
if (missing_n > 0) {
message("‚ö† Dropping ", missing_n, " rows with missing Longitude/Latitude")
rats_clean <- rats_clean %>%
filter(!is.na(Longitude), !is.na(Latitude))
}
# 2) Convert to sf point geometry
rat_sf <- st_as_sf(
rats_clean,
coords = c("Longitude", "Latitude"),
crs    = 4326,
remove = FALSE
)
# 3) Read PLUTO parcels and repair invalid geometries
bbl_sf <- st_read("data/raw/MapPLUTO.shp") %>%
st_transform(crs = st_crs(rat_sf))
if (!all(st_is_valid(bbl_sf))) {
message("üîß Repairing invalid PLUTO geometries‚Ä¶")
bbl_sf <- sf::st_make_valid(bbl_sf)
}
# 4) Spatial-join: stamp each sighting with its BBL
rat_bbl <- st_join(
rat_sf,
bbl_sf %>% select(BBL),
join = st_within,
left = TRUE
)
# 5) Read in PLUTO & ACS tables (keyed by BBL)
pluto <- read_csv("data/raw/PLUTO.csv")
# 5) Read in PLUTO parcels (MapPLUTO.shp) for attributes
message("üîÑ Reading PLUTO shapefile for attributes‚Ä¶")
pluto_sf <- st_read("data/raw/MapPLUTO.shp")
# 5b) Drop geometry to get the flat attribute table
pluto <- pluto_sf %>%
st_drop_geometry()
# 6) Read in ACS table (keyed by BBL)
message("üîÑ Reading ACS√óBBL CSV‚Ä¶")
acs <- read_csv("data/raw/ACS.csv")
# 7) Attribute-join covariates to each sighting
rat_enriched <- rat_bbl %>%
left_join(pluto, by = "BBL") %>%
left_join(acs,   by = "BBL")
View(rat_bbl)
rlang::last_trace()
View(rat_bbl)
View(rats_clean)
View(rat_bbl)
# 0) Load required packages
library(sf)      # spatial data handling
library(dplyr)   # data wrangling
library(readr)   # CSV import
# 1) Read in your cleaned rats dataset
rats_clean <- read_csv("data/processed/rats_clean.csv")
# 1b) Drop any rows with missing coords
missing_n <- rats_clean %>%
filter(is.na(Longitude) | is.na(Latitude)) %>%
nrow()
if (missing_n > 0) {
message("‚ö† Dropping ", missing_n, " rows with missing Longitude/Latitude")
rats_clean <- rats_clean %>%
filter(!is.na(Longitude), !is.na(Latitude))
}
# 2) Convert to sf point geometry
rat_sf <- st_as_sf(
rats_clean,
coords = c("Longitude", "Latitude"),
crs    = 4326,
remove = FALSE
)
# 3) Read PLUTO parcels and repair invalid geometries
bbl_sf <- st_read("data/raw/MapPLUTO.shp") %>%
st_transform(crs = st_crs(rat_sf))
if (!all(st_is_valid(bbl_sf))) {
message("üîß Repairing invalid PLUTO geometries‚Ä¶")
bbl_sf <- sf::st_make_valid(bbl_sf)
}
# 4) Spatial‚Äêjoin, pulling BBL in under a temp name to avoid collision
rat_bbl <- st_join(
rat_sf,
bbl_sf %>% select(join_BBL = BBL),
join = st_within,
left = TRUE
) %>%
rename(BBL = join_BBL)
rlang::last_trace()
View(rat_bbl)
View(rats_clean)
pluto <- st_read("data/raw/MapPLUTO.shp") %>%
st_drop_geometry()
View(api)
view(census_api_key())
# 0) Load required packages
library(sf)      # spatial data handling
library(dplyr)   # data wrangling
library(readr)   # CSV import
# 1) Read in your cleaned rats dataset (kept intact)
rats_clean <- read_csv("data/processed/rats_clean.csv")
# 2) Create a join‚Äêready copy and drop its old BBL
rats_clean_join <- rats_clean %>%
select(-BBL)
# 2b) Drop any rows in the join copy with missing coords
missing_n <- rats_clean_join %>%
filter(is.na(Longitude) | is.na(Latitude)) %>%
nrow()
if (missing_n > 0) {
message("‚ö† Dropping ", missing_n, " rows with missing Longitude/Latitude")
rats_clean_join <- rats_clean_join %>%
filter(!is.na(Longitude), !is.na(Latitude))
}
# 3) Convert the join copy to sf point geometry
rat_sf <- st_as_sf(
rats_clean_join,
coords = c("Longitude", "Latitude"),
crs    = 4326,
remove = FALSE
)
# 4) Read PLUTO parcels and repair invalid geometries
bbl_sf <- st_read("data/raw/MapPLUTO.shp") %>%
st_transform(crs = st_crs(rat_sf))
if (!all(st_is_valid(bbl_sf))) {
message("üîß Repairing invalid PLUTO geometries‚Ä¶")
bbl_sf <- sf::st_make_valid(bbl_sf)
}
# 5) Spatial‚Äêjoin: bring in the parcel BBL under a temp name
rat_bbl <- st_join(
rat_sf,
bbl_sf %>% select(join_BBL = BBL),
join = st_within,
left = TRUE
) %>%
rename(BBL = join_BBL)
# 6) Read PLUTO attributes (directly from shapefile) and drop geometry
message("üîÑ Reading PLUTO shapefile for attributes‚Ä¶")
pluto <- st_read("data/raw/MapPLUTO.shp") %>%
st_drop_geometry()
# 7) Read in ACS table (keyed by BBL)
message("üîÑ Reading ACS√óBBL CSV‚Ä¶")
acs <- read_csv("data/raw/ACS.csv")
# 8) Attribute‚Äêjoin covariates to each sighting
rat_enriched <- rat_bbl %>%
left_join(pluto, by = "BBL") %>%
left_join(acs,   by = "BBL")
# 9) Ensure outputs folder exists
if (!dir.exists("outputs")) dir.create("outputs", recursive = TRUE)
# 10) Write out enriched GeoJSON
st_write(
rat_enriched,
"outputs/rat_clean.geojson",
driver     = "GeoJSON",
delete_dsn = TRUE
)
message("‚úÖ Spatial join complete: outputs/rat_clean.geojson written")
