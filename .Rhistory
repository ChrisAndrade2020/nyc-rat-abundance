sprintf("4%02d", 1:14),   # Queens    401-414
sprintf("5%02d", 1:3)     # Staten Is 501-503
)
# 1 ─ BG-level ACS (pop & income, one row per block-group)
bg <- read_csv("output/bg_calls_ACS_summary.csv", show_col_types = FALSE) %>%
mutate(GEOID = as.character(GEOID))
# 2 ─ Mapping GEOID → CD_ID once (no duplication)
map <- read_csv("output/rat_with_bg_ACS_point.csv", show_col_types = FALSE) %>%
select(GEOID, CD_ID, Borough) %>%
distinct()
# 3 ─ Join, keep only official 59 CDs
bg_map <- bg %>%
inner_join(map, by = "GEOID") %>%
filter(CD_ID %in% valid_cd)
# 051_cd_9box_prep.R  –  FINAL “59-row / no-NA” version
# -------------------------------------------------------
# Inputs
#   • output/bg_calls_ACS_summary.csv      (BG pop + income, 1 row/BG)
#   • output/rat_with_bg_ACS_point.csv     (rat tickets with GEOID + CD_ID)
# Outputs
#   • data/processed/cd_9box_points.csv    (59 rows, clean)
#   • data/processed/cd_9box_matrix.csv    (3 × 3 counts)
# -------------------------------------------------------
library(dplyr)
library(readr)
library(tidyr)
N_QTR <- 58  # 2010-Q1 ➝ 2025-Q2 inclusive
valid_cd <- c(
sprintf("1%02d", 1:12),  # Manhattan 101-112
sprintf("2%02d", 1:12),  # Bronx     201-212
sprintf("3%02d", 1:18),  # Brooklyn  301-318
sprintf("4%02d", 1:14),  # Queens    401-414
sprintf("5%02d", 1:3)    # Staten Is 501-503
)
# 1 ── BG-level ACS (each BG once) ---------------------------------------------
bg <- read_csv("output/bg_calls_ACS_summary.csv",
show_col_types = FALSE) %>%
mutate(GEOID = as.character(GEOID))          # *** string ***
# 2 ── GEOID → CD_ID mapping (one row/BG) --------------------------------------
map <- read_csv("output/rat_with_bg_ACS_point.csv",
show_col_types = FALSE) %>%
mutate(GEOID = as.character(GEOID)) %>%     # *** string ***
select(GEOID, CD_ID, Borough) %>%
distinct()
# 3 ── Join & keep only the 59 official CDs ------------------------------------
bg_map <- bg %>%
inner_join(map, by = "GEOID") %>%
filter(CD_ID %in% valid_cd)
# 4 ── Pop-weighted income & population per CD ---------------------------------
cd_demo <- bg_map %>%
group_by(CD_ID, Borough) %>%
summarise(
pop_total  = sum(pop_tot, na.rm = TRUE),
Med_Income = sum(med_income * pop_tot, na.rm = TRUE) /
sum(pop_tot[!is.na(med_income)], na.rm = TRUE),
.groups = "drop"
)
# 5 ── Distinct rat-call counts per CD -----------------------------------------
calls <- read_csv("output/rat_with_bg_ACS_point.csv",
show_col_types = FALSE) %>%
filter(CD_ID %in% valid_cd) %>%
count(CD_ID, name = "calls_total")
# 6 ── Merge demos + calls & compute rate --------------------------------------
cd <- cd_demo %>%
left_join(calls, by = "CD_ID") %>%
mutate(
calls_total            = replace_na(calls_total, 0),
Calls_Per_Qtr_Per_10K  = (calls_total / pop_total * 10000) / N_QTR
)
stopifnot(nrow(cd) == 59)   # → will error if not exactly 59 rows
# 6 ── Merge demos + calls & compute rate --------------------------------------
cd <- cd_demo %>%
left_join(calls, by = "CD_ID") %>%
mutate(
calls_total            = replace_na(calls_total, 0),
Calls_Per_Qtr_Per_10K  = (calls_total / pop_total * 10000) / N_QTR
)
# 7 ── Tertile bins (always Low / Med / High) ----------------------------------
cd <- cd %>%
mutate(
Income_Bin = factor(ntile(Med_Income, 3),
levels = 1:3,
labels = c("Low Income", "Med Income", "High Income")),
Rate_Bin   = factor(ntile(Calls_Per_Qtr_Per_10K, 3),
levels = 1:3,
labels = c("Low Rate", "Med Rate", "High Rate"))
)
# 8 ── Write outputs -----------------------------------------------------------
write_csv(cd, "data/processed/cd_9box_points.csv")
message("✅ cd_9box_points.csv rows: ", nrow(cd))
matrix3 <- cd %>%
count(Income_Bin, Rate_Bin, name = "n_CD") %>%
complete(Income_Bin, Rate_Bin, fill = list(n_CD = 0))
write_csv(matrix3, "data/processed/cd_9box_matrix.csv")
message("✅ cd_9box_matrix.csv created")
View(cd)
gc()
# ───────────────────────────────────────────────────────────
# Script: 034_spike_inspection.R
# Purpose: Identify the single highest-call day per CD_ID and its hotspot address
# Inputs:  output/rats_enriched.geojson
# Outputs: output/kpi_spike_summary.csv
# Depends: sf, dplyr, lubridate, readr
# ───────────────────────────────────────────────────────────
# 1. Load spatial and data-wrangling libraries
library(sf)
library(dplyr)
library(lubridate)
library(readr)
# 2. Read enriched GeoJSON, drop geometry, and normalize column names
rats <- st_read("output/rats_enriched.geojson", quiet = TRUE) %>%
st_drop_geometry() %>%
rename_with(~ gsub("\\.", "_", .x), everything())
# ───────────────────────────────────────────────────────────
# Script: master_run_all.R
# Purpose: Run the entire NYC rat-data pipeline in order and verify all expected artefacts exist
# Inputs:  R scripts under scripts/ (000_setup_cmdstanr.R, 011_data_prep.R, …, 043_funnel.R)
# Outputs: data/processed/*.csv, output/*.csv & rats_enriched.geojson
# Depends: base R, dplyr, readr, tidyr
# ───────────────────────────────────────────────────────────
## 0) (Optional) Set working directory to repo root ----------------------------
# setwd("C:/path/to/nyc-rat-abundance")
## 1) Ordered vector of scripts to source ---------------------------------------
scripts <- c(
# Batch 0 · Environment setup
"scripts/000_setup_cmdstanr.R",
# Batch 1 · Data ingest & cleaning
"scripts/011_data_prep.R",
"scripts/012_data_prep_derivation.R",
"scripts/021_acs_fetch.R",
"scripts/022_spatial_join.R",
"scripts/024_bg_acs_join.R",
# Batch 2 · Analysis & modelling outputs
"scripts/023_income_scatter.R",
"scripts/031_qhcr_model.R",
"scripts/032_generate_daily_counts.R",
"scripts/033_generate_quarterly_counts.R",
"scripts/034_spike_inspection.R",
"scripts/035_winsorize_daily_counts.R",
"scripts/036_deciles.R",
"scripts/041_rat_pop_est.R",
"scripts/042_rat_pop_boro.R",
"scripts/043_funnel.R"
)
## 2) Source each script, echoing code and stopping on error -------------------
for (s in scripts) {
message("🔄  Running ", s, " …")
source(s, echo = TRUE, max.deparse.length = 200)
}
## 3) Verify all expected artefacts exist --------------------------------------
expected <- c(
# processed data
"data/processed/rats_clean.csv",
"data/processed/rats_ready.csv",
"data/processed/borough_rates.csv",
"data/processed/ACS.csv",
# enriched and spatial outputs
"output/rats_enriched.geojson",
"output/rat_with_bg_ACS_point.csv",
"output/bg_calls_ACS_summary.csv",
# analysis tables
"output/income_scatter.csv",
"output/daily_rats_by_cd_win.csv",
"output/quarterly_rats_by_cd.csv",
"output/kpi_spike_summary.csv",
"output/decile_call_rates_vfinal.csv",
"output/citywide_call_rate_qtr.csv",
# modelling & population estimates
"output/qhcr_draws.rds",
"output/district_qhcr_v2.csv",
"output/yearly_rat_population.csv",
"output/boro_yearly_rat_population.csv",
# funnel analysis
"output/rat_funnel.csv"
)
missing <- expected[!file.exists(expected)]
if (length(missing) > 0) {
stop("❌ Missing outputs: ", paste(missing, collapse = ", "))
}
message("🎉 All scripts ran successfully and all expected artefacts are in place!")
# ───────────────────────────────────────────────────────────
# Script: 036_deciles.R
# Purpose:
#   1. Compute median call rates per income decile (per quarter)
#   2. Compute citywide median and mean call rates per quarter
#   3. Compute average + total calls per income decile × quarter
# Inputs:
#   • output/bg_calls_ACS_summary.csv
#   • output/rats_enriched.geojson
# Outputs:
#   • output/decile_call_rates_vfinal.csv
#   • output/citywide_call_rate_qtr.csv
#   • output/decile_call_counts.csv
# Depends: sf, dplyr, readr, lubridate
# ───────────────────────────────────────────────────────────
library(sf)
library(dplyr)
library(readr)
library(lubridate)
# — Paths & parameters -------------------------------------
rats_path  <- "output/rats_enriched.geojson"
bg_path    <- "output/bg_calls_ACS_summary.csv"
out_dir    <- "output"
n_quarters <- 58  # quarters from 2010–Q1 to 2025–Q2
# 1. Load and prep BG summary for deciles ---------------
bg <- read_csv(bg_path, show_col_types = FALSE) %>%
mutate(
GEOID                = as.character(GEOID),
income_decile        = ntile(med_income, 10),
rate_per_qtr_per_10k = rate_per_10k / n_quarters
) %>%
select(GEOID, income_decile, rate_per_qtr_per_10k, pop_tot)
# 2. Compute & write median rate per decile -------------
decile_rates <- bg %>%
group_by(income_decile) %>%
summarise(
median_rate_per_qtr = median(rate_per_qtr_per_10k, na.rm = TRUE),
.groups = "drop"
)
write_csv(decile_rates, file.path(out_dir, "decile_call_rates_vfinal.csv"))
# ── 3. City-wide per-quarter stats (deduped tickets) ─────────────────────────
rats <- st_read(rats_path, quiet = TRUE) %>%
st_drop_geometry() %>%
distinct(`Unique Key`, .keep_all = TRUE) %>%     # ← deduplicate once here
mutate(
created_dt = ymd_hms(created_dt),
quarter    = paste0(year(created_dt), "-Q", quarter(created_dt))
)
View(decile_rates)
# ───────────────────────────────────────────────────────────
# Script: 034_spike_inspection.R
# Purpose: Identify the single highest-call day per CD_ID and its hotspot address
# Inputs:  output/rats_enriched.geojson
# Outputs: output/kpi_spike_summary.csv
# Depends: sf, dplyr, lubridate, readr
# ───────────────────────────────────────────────────────────
# 1. Load spatial and data-wrangling libraries
library(sf)
library(dplyr)
library(lubridate)
library(readr)
# 2. Read enriched GeoJSON, drop geometry, and normalize column names
rats <- st_read("output/rats_enriched.geojson", quiet = TRUE) %>%
st_drop_geometry() %>%
rename_with(~ gsub("\\.", "_", .x), everything())
# 3. Compute daily distinct-ticket counts by CD_ID
daily_cd <- rats %>%
mutate(report_date = as_date(ymd_hms(created_dt))) %>%
filter(!is.na(CD_ID)) %>%
group_by(CD_ID, report_date) %>%
summarise(calls = n_distinct(Unique_Key), .groups = "drop")
# 4. Find the single CD_ID-day with the highest call count
max_day <- daily_cd %>% slice_max(calls, n = 1)
# 5. Within that CD_ID and date, identify the most-called Address
hotspot <- rats %>%
mutate(report_date = as_date(ymd_hms(created_dt))) %>%
filter(
report_date == max_day$report_date,
CD_ID       == max_day$CD_ID
) %>%
count(Address) %>%
slice_max(n, n = 1) %>%
rename(
hotspot_address = Address,
hotspot_calls   = n
)
# 6. Compile KPI row and write out summary CSV
kpi <- tibble(
max_calls       = max_day$calls,
max_date        = as.character(max_day$report_date),
max_CD_ID       = max_day$CD_ID,
hotspot_address = hotspot$hotspot_address,
hotspot_calls   = hotspot$hotspot_calls
)
if (!dir.exists("output")) dir.create("output", recursive = TRUE)
write_csv(kpi, "output/kpi_spike_summary.csv")
message("Wrote KPI spike summary to output/kpi_spike_summary.csv")
View(kpi)
# ───────────────────────────────────────────────────────────
# Script: 036_deciles.R
# Purpose:
#   1. Compute median call rates per income decile (per quarter)
#   2. Compute citywide median and mean call rates per quarter
#   3. Compute average + total calls per income decile × quarter
# Inputs:
#   • output/bg_calls_ACS_summary.csv
#   • output/rats_enriched.geojson
# Outputs:
#   • output/decile_call_rates_vfinal.csv
#   • output/citywide_call_rate_qtr.csv
#   • output/decile_call_counts.csv
# Depends: sf, dplyr, readr, lubridate
# ───────────────────────────────────────────────────────────
library(sf)
library(dplyr)
library(readr)
library(lubridate)
# — Paths & parameters -------------------------------------
rats_path  <- "output/rats_enriched.geojson"
bg_path    <- "output/bg_calls_ACS_summary.csv"
out_dir    <- "output"
n_quarters <- 58  # quarters from 2010–Q1 to 2025–Q2
# 1. Load and prep BG summary for deciles ---------------
bg <- read_csv(bg_path, show_col_types = FALSE) %>%
mutate(
GEOID                = as.character(GEOID),
income_decile        = ntile(med_income, 10),
rate_per_qtr_per_10k = rate_per_10k / n_quarters
) %>%
select(GEOID, income_decile, rate_per_qtr_per_10k, pop_tot)
# 2. Compute & write median rate per decile -------------
decile_rates <- bg %>%
group_by(income_decile) %>%
summarise(
median_rate_per_qtr = median(rate_per_qtr_per_10k, na.rm = TRUE),
.groups = "drop"
)
write_csv(decile_rates, file.path(out_dir, "decile_call_rates_vfinal.csv"))
# 3. Compute & write citywide summary -------------------
rats <- st_read(rats_path, quiet = TRUE) %>%
st_drop_geometry() %>%
distinct(`Unique.Key`, .keep_all = TRUE) %>%   # ← dedupe by ticket
mutate(
created_dt = ymd_hms(created_dt),
quarter    = paste0(year(created_dt), "-Q", quarter(created_dt))
)
city_pop_10k <- sum(bg$pop_tot, na.rm = TRUE) / 1e4
city_qtr <- rats %>%
group_by(quarter) %>%
summarise(calls = n(), .groups = "drop") %>%
mutate(rate_per_10k = calls / city_pop_10k)
city_stats <- city_qtr %>%
summarise(
median_rate_per_qtr = median(rate_per_10k, na.rm = TRUE),
mean_rate_per_qtr   = mean(rate_per_10k,   na.rm = TRUE),
.groups = "drop"
)
write_csv(city_stats, file.path(out_dir, "citywide_call_rate_qtr.csv"))
# 4. Compute & write decile × quarter call counts -------
rats2 <- rats %>%                              # already de-duplicated
mutate(GEOID = as.character(GEOID)) %>%
left_join(bg %>% select(GEOID, income_decile), by = "GEOID")
decile_counts <- rats2 %>%
group_by(income_decile, quarter) %>%
summarise(calls = n(), .groups = "drop") %>%
group_by(income_decile) %>%
summarise(
avg_calls_per_quarter = mean(calls),
total_calls           = sum(calls),
.groups = "drop"
)
write_csv(decile_counts, file.path(out_dir, "decile_call_counts.csv"))
message("✅ Wrote decile rates, city stats, and decile call counts to ", out_dir)
View(decile_counts)
View(decile_rates)
View(decile_counts)
# ───────────────────────────────────────────────────────────
# Script: 036_deciles.R
# Purpose:
#   1. Compute median call rates per income decile (per quarter)
#   2. Compute citywide median and mean call rates per quarter
#   3. Compute average + total calls per income decile × quarter
# Inputs:
#   • output/bg_calls_ACS_summary.csv
#   • output/rats_enriched.geojson
# Outputs:
#   • output/decile_call_rates_vfinal.csv
#   • output/citywide_call_rate_qtr.csv
#   • output/decile_call_counts.csv
# Depends: sf, dplyr, readr, lubridate
# ───────────────────────────────────────────────────────────
library(sf)
library(dplyr)
library(readr)
library(lubridate)
# — Paths & parameters -------------------------------------
rats_path  <- "output/rats_enriched.geojson"
bg_path    <- "output/bg_calls_ACS_summary.csv"
out_dir    <- "output"
n_quarters <- 58  # quarters from 2010–Q1 to 2025–Q2
# 1. Load and prep BG summary for deciles ---------------
bg <- read_csv(bg_path, show_col_types = FALSE) %>%
mutate(
GEOID                = as.character(GEOID),
income_decile        = ntile(med_income, 10),
rate_per_qtr_per_10k = rate_per_10k / n_quarters
) %>%
select(GEOID, income_decile, rate_per_qtr_per_10k, pop_tot) %>%
filter(!is.na(income_decile))   # drop any BGs that failed to get a decile
# 2. Compute & write median rate per decile -------------
decile_rates <- bg %>%
group_by(income_decile) %>%
summarise(
median_rate_per_qtr = median(rate_per_qtr_per_10k, na.rm = TRUE),
.groups = "drop"
)
write_csv(decile_rates, file.path(out_dir, "decile_call_rates_vfinal.csv"))
# 3. Compute & write citywide summary -------------------
rats <- st_read(rats_path, quiet = TRUE) %>%
st_drop_geometry() %>%
distinct(`Unique.Key`, .keep_all = TRUE) %>%   # dedupe tickets
mutate(
created_dt = ymd_hms(created_dt),
quarter    = paste0(year(created_dt), "-Q", quarter(created_dt))
)
city_pop_10k <- sum(bg$pop_tot, na.rm = TRUE) / 1e4
city_qtr <- rats %>%
group_by(quarter) %>%
summarise(calls = n(), .groups = "drop") %>%
mutate(rate_per_10k = calls / city_pop_10k)
city_stats <- city_qtr %>%
summarise(
median_rate_per_qtr = median(rate_per_10k, na.rm = TRUE),
mean_rate_per_qtr   = mean(rate_per_10k,   na.rm = TRUE),
.groups = "drop"
)
write_csv(city_stats, file.path(out_dir, "citywide_call_rate_qtr.csv"))
# 4. Compute & write decile × quarter call counts -------
rats2 <- rats %>%                              # reuse deduped tickets
mutate(GEOID = as.character(GEOID)) %>%
left_join(bg %>% select(GEOID, income_decile), by = "GEOID") %>%
filter(!is.na(income_decile))                # drop any tickets without a decile
decile_counts <- rats2 %>%
group_by(income_decile, quarter) %>%
summarise(calls = n(), .groups = "drop") %>%
group_by(income_decile) %>%
summarise(
avg_calls_per_quarter = mean(calls),
total_calls           = sum(calls),
.groups = "drop"
)
write_csv(decile_counts, file.path(out_dir, "decile_call_counts.csv"))
message("✅ Wrote decile rates, city stats, and decile call counts to ", out_dir)
View(decile_counts)
View(decile_rates)
View(decile_counts)
# ───────────────────────────────────────────────────────────
# Script: 036_deciles.R
# Purpose:
#   1. Compute median call rates per income decile (per quarter)
#   2. Compute citywide median and mean call rates per quarter
#   3. Compute average + total calls per income decile × quarter
# Inputs:
#   • output/rats_enriched.geojson
# Outputs:
#   • output/decile_call_rates_vfinal.csv
#   • output/citywide_call_rate_qtr.csv
#   • output/decile_call_counts.csv
# Depends: sf, dplyr, readr, lubridate
# ───────────────────────────────────────────────────────────
library(sf)
library(dplyr)
library(readr)
library(lubridate)
# — Paths & parameters -------------------------------------
rats_path  <- "output/rats_enriched.geojson"
out_dir    <- "output"
n_quarters <- 58  # Q1 2010 → Q2 2025
# ── Load and dedupe rat tickets, assign deciles ──────────
rats <- st_read(rats_path, quiet = TRUE) %>%
st_drop_geometry() %>%
distinct(`Unique.Key`, .keep_all = TRUE) %>%       # one row per ticket
filter(!is.na(med_income), pop_tot > 0) %>%        # drop any missing income or pop0
mutate(
income_decile = ntile(med_income, 10),
created_dt    = ymd_hms(created_dt),
quarter       = paste0(year(created_dt), "-Q", quarter(created_dt))
)
# ── 1) Median rate per decile (calls per 10k residents per quarter) ────────
decile_rates <- rats %>%
group_by(income_decile, quarter) %>%
summarise(
calls       = n(),
pop_total   = sum(pop_tot, na.rm = TRUE),  # total pop of all parcels for that quarter
.groups     = "drop"
) %>%
mutate(rate_per_qtr_per_10k = calls / pop_total * 10000) %>%
group_by(income_decile) %>%
summarise(
median_rate_per_qtr = median(rate_per_qtr_per_10k, na.rm = TRUE),
.groups             = "drop"
)
write_csv(decile_rates, file.path(out_dir, "decile_call_rates_vfinal.csv"))
# ── 2) Citywide weekly/quarterly stats ──────────────────────────────────────
city_pop_10k <- sum(rats$pop_tot, na.rm = TRUE) / 1e4
city_qtr <- rats %>%
group_by(quarter) %>%
summarise(calls = n(), .groups = "drop") %>%
mutate(rate_per_10k = calls / city_pop_10k)
city_stats <- city_qtr %>%
summarise(
median_rate_per_qtr = median(rate_per_10k, na.rm = TRUE),
mean_rate_per_qtr   = mean(rate_per_10k,   na.rm = TRUE),
.groups             = "drop"
)
write_csv(city_stats, file.path(out_dir, "citywide_call_rate_qtr.csv"))
# ── 3) Average + total calls per decile × quarter ──────────────────────────
decile_counts <- rats %>%
group_by(income_decile, quarter) %>%
summarise(calls = n(), .groups = "drop") %>%
group_by(income_decile) %>%
summarise(
avg_calls_per_quarter = mean(calls),
total_calls           = sum(calls),
.groups               = "drop"
)
write_csv(decile_counts, file.path(out_dir, "decile_call_counts.csv"))
message("✅ Wrote decile rates, city stats, and decile call counts to ", out_dir)
View(decile_counts)
View(decile_rates)
View(decile_counts)
View(rats2)
