st_drop_geometry()
# 3. Compute daily distinct-ticket counts by CD_ID
daily_cd <- rats %>%
mutate(report_date = as_date(ymd_hms(created_dt))) %>%
filter(!is.na(CD_ID)) %>%
group_by(CD_ID, report_date) %>%
summarise(calls = n_distinct(Unique.Key), .groups = "drop")
# 4. Write out the daily counts CSV
write_csv(daily_cd, "output/daily_rats_by_cd.csv")
message("Wrote daily counts to output/daily_rats_by_cd.csv")
# ───────────────────────────────────────────────────────────
# Script: 033_generate_quarterly_counts.R
# Purpose: Aggregate enriched rat sightings into quarterly counts per community district
# Inputs:  output/rats_enriched.geojson
# Outputs: output/quarterly_rats_by_cd.csv
# Depends: sf, dplyr, lubridate, readr
# ───────────────────────────────────────────────────────────
# 1. Load spatial and data-wrangling libraries
library(sf)
library(dplyr)
library(lubridate)
library(readr)
# 2. Read enriched GeoJSON, drop geometry, filter valid CD_IDs, assign quarter
rats <- st_read("output/rats_enriched.geojson", quiet = TRUE) %>%
st_drop_geometry() %>%
filter(!is.na(CD_ID)) %>%
mutate(quarter = floor_date(ymd_hms(created_dt), unit = "quarter"))
# 3. Summarise calls per CD_ID × quarter
quarterly_cd <- rats %>%
group_by(CD_ID, quarter) %>%
summarise(calls = n(), .groups = "drop")
# 4. Write out the quarterly counts CSV
write_csv(quarterly_cd, "output/quarterly_rats_by_cd.csv")
message("Wrote quarterly counts to output/quarterly_rats_by_cd.csv")
View(daily_cd)
View(quarterly_cd)
# ───────────────────────────────────────────────────────────
# Script: 034_spike_inspection.R
# Purpose: Identify the single highest-call day per CD_ID and its hotspot address
# Inputs:  output/rats_enriched.geojson
# Outputs: output/kpi_spike_summary.csv
# Depends: sf, dplyr, lubridate, readr
# ───────────────────────────────────────────────────────────
# 1. Load spatial and data-wrangling libraries
library(sf)
library(dplyr)
library(lubridate)
library(readr)
# 2. Read enriched GeoJSON, drop geometry, and normalize column names
rats <- st_read("output/rats_enriched.geojson", quiet = TRUE) %>%
st_drop_geometry() %>%
rename_with(~ gsub("\\.", "_", .x), everything())
# 3. Compute daily distinct-ticket counts by CD_ID
daily_cd <- rats %>%
mutate(report_date = as_date(ymd_hms(created_dt))) %>%
filter(!is.na(CD_ID)) %>%
group_by(CD_ID, report_date) %>%
summarise(calls = n_distinct(Unique_Key), .groups = "drop")
# 4. Find the single CD_ID-day with the highest call count
max_day <- daily_cd %>% slice_max(calls, n = 1)
# 5. Within that CD_ID and date, identify the most-called Address
hotspot <- rats %>%
mutate(report_date = as_date(ymd_hms(created_dt))) %>%
filter(
report_date == max_day$report_date,
CD_ID       == max_day$CD_ID
) %>%
count(Address) %>%
slice_max(n, n = 1) %>%
rename(
hotspot_address = Address,
hotspot_calls   = n
)
# 6. Compile KPI row and write out summary CSV
kpi <- tibble(
max_calls       = max_day$calls,
max_date        = as.character(max_day$report_date),
max_CD_ID       = max_day$CD_ID,
hotspot_address = hotspot$hotspot_address,
hotspot_calls   = hotspot$hotspot_calls
)
if (!dir.exists("output")) dir.create("output", recursive = TRUE)
write_csv(kpi, "output/kpi_spike_summary.csv")
message("Wrote KPI spike summary to output/kpi_spike_summary.csv")
View(kpi)
# ───────────────────────────────────────────────────────────
# Script: 035_winsorize_daily_counts.R
# Purpose: Cap (winsorize) daily counts at the 99th percentile for visualization
# Inputs:  output/daily_rats_by_cd.csv
# Outputs: output/daily_rats_by_cd_win.csv
# Depends: dplyr, readr
# ───────────────────────────────────────────────────────────
# 1. Load data-wrangling libraries
library(dplyr)
library(readr)
# 2. Read daily counts CSV
daily_cd <- read_csv("output/daily_rats_by_cd.csv", show_col_types = FALSE)
# 3. Determine the winsorization threshold (99th percentile)
P      <- 0.99
cap_val <- quantile(daily_cd$calls, P, na.rm = TRUE)
message("Capping calls at the ", P*100, "th percentile: ", cap_val)
# 4. Add capped value and flag to each row
daily_cd <- daily_cd %>%
mutate(
calls_capped = pmin(calls, cap_val),
capped_flag  = calls > cap_val
)
# 5. Write out the winsorized counts CSV
write_csv(daily_cd, "output/daily_rats_by_cd_win.csv")
message("Wrote winsorized daily counts to output/daily_rats_by_cd_win.csv")
View(daily_cd)
# ───────────────────────────────────────────────────────────
# Script: 036_deciles.R
# Purpose:
#   1. Compute median call rates per income decile (per quarter)
#   2. Compute citywide median and mean call rates per quarter
# Inputs:
#   • output/bg_calls_ACS_summary.csv
#   • output/rats_enriched.geojson
# Outputs:
#   • output/decile_call_rates_vfinal.csv
#   • output/citywide_call_rate_qtr.csv
#   • output/decile_call_counts.csv
# Depends: sf, dplyr, readr, lubridate
# ───────────────────────────────────────────────────────────
# 1. Load spatial and data-wrangling libraries
library(sf)
library(dplyr)
library(readr)
library(lubridate)
# 2. Define file paths and parameters
rats_path    <- "output/rats_enriched.geojson"
bg_path      <- "output/bg_calls_ACS_summary.csv"
out_dir      <- "output"
n_quarters   <- 58  # number of quarters from 2010 through May 2025
# 3. Load and prepare block-group summary
bg <- read_csv(bg_path, show_col_types = FALSE) %>%
mutate(
GEOID                = as.character(GEOID),
income_decile        = ntile(med_income, 10),
rate_per_qtr_per_10k = rate_per_10k / n_quarters
) %>%
select(GEOID, income_decile, rate_per_qtr_per_10k, pop_tot)
# 4. Compute and save median rate per decile
decile_rates <- bg %>%
group_by(income_decile) %>%
summarise(
median_rate_per_qtr = median(rate_per_qtr_per_10k, na.rm = TRUE),
.groups = "drop"
)
decile_csv <- file.path(out_dir, "decile_call_rates_vfinal.csv")
write_csv(decile_rates, decile_csv)
# 5. Load enriched sightings, derive quarters, compute citywide rates
rats <- st_read(rats_path, quiet = TRUE) %>%
st_drop_geometry() %>%
mutate(
created_dt = ymd_hms(created_dt),
quarter    = paste0(year(created_dt), "-Q", quarter(created_dt))
)
city_pop_10k <- sum(bg$pop_tot, na.rm = TRUE) / 1e4
city_qtr <- rats %>%
group_by(quarter) %>%
summarise(calls = n(), .groups = "drop") %>%
mutate(rate_per_10k = calls / city_pop_10k)
city_stats <- city_qtr %>%
summarise(
median_rate_per_qtr = median(rate_per_10k, na.rm = TRUE),
mean_rate_per_qtr   = mean(rate_per_10k,   na.rm = TRUE),
.groups = "drop"
)
city_csv <- file.path(out_dir, "citywide_call_rate_qtr.csv")
write_csv(city_stats, city_csv)
# 6. Console output of results
message("✅ Wrote decile rates to: ", decile_csv)
print(decile_rates)
message("\n✅ Wrote citywide rates to: ", city_csv)
print(city_stats)
# 7. (Duplicate block – builds true decile call counts and saves another CSV)
library(dplyr)
library(readr)
library(lubridate)
rats <- read_csv("output/rats_enriched.geojson") %>%
mutate(
created_dt = ymd_hms(Created),
quarter    = paste0(year(), "-Q", quarter(Created))
)
View(rats_enriched)
# 7. (Duplicate block – builds true decile call counts and saves another CSV)
library(dplyr)
library(readr)
library(lubridate)
rats <- read_csv("output/rats_enriched.geojson") %>%
mutate(
created_dt = ymd_hms(Created),
quarter    = paste0(year(), "-Q", quarter(created_dt))
)
# ───────────────────────────────────────────────────────────
# Script: 041_rat_pop_est.R
# Purpose: Generate yearly rat-population indices and absolute counts from QHCR model draws
# Inputs:
#   • output/qhcr_draws.rds
#   • output/quarter_effects.csv
# Outputs:
#   • output/yearly_rat_population.csv
# Depends: readr, dplyr, lubridate, posterior, tidyr
# ───────────────────────────────────────────────────────────
# 1. Load libraries
library(readr)
library(dplyr)
library(lubridate)
library(posterior)
library(tidyr)
# 2. Read posterior draws and quarter metadata
draws     <- read_rds("output/qhcr_draws.rds")
occasions <- read_csv("output/quarter_effects.csv", show_col_types = FALSE)$quarter
# 3. Compute city_mean per draw from lambda
lambda_draws <- draws %>%
select(starts_with("lambda[")) %>%
mutate(draw = row_number()) %>%
pivot_longer(-draw, names_to="param", values_to="log_lambda") %>%
mutate(lambda = exp(log_lambda)) %>%
group_by(draw) %>%
summarise(city_mean = mean(lambda), .groups="drop")
# 4. Expand delta draws, map to quarters, compute per-quarter abundance
delta_draws <- draws %>%
select(starts_with("delta[")) %>%
mutate(draw = row_number()) %>%
pivot_longer(-draw, names_to="param", values_to="log_delta") %>%
mutate(
q_idx   = as.integer(gsub(".*\\[(\\d+)\\].*", "\\1", param)),
quarter = as.Date(occasions[q_idx]),
factor  = exp(log_delta),
year    = year(quarter)
)
city_q <- delta_draws %>%
left_join(lambda_draws, by="draw") %>%
mutate(q_abund = city_mean * factor)
# 5. Compute yearly mean and index relative to 2010 per draw
yearly_draw <- city_q %>%
group_by(draw, year) %>%
summarise(mean_qhcr = mean(q_abund), .groups="drop") %>%
group_by(draw) %>%
mutate(rat_index = mean_qhcr / mean_qhcr[year == 2010]) %>%
ungroup()
# 6. Summarise posterior quantiles per year
yearly_ci <- yearly_draw %>%
group_by(year) %>%
summarise(
rat_index      = mean(rat_index),
rat_index_low  = quantile(rat_index, 0.025),
rat_index_high = quantile(rat_index, 0.975),
.groups        = "drop"
) %>%
filter(year <= 2024)
# 7. Convert index to absolute rat counts (anchor 2010 = 2M)
anchor_rats <- 2e6
yearly_out <- yearly_ci %>%
mutate(
estimated_rats = rat_index      * anchor_rats,
rats_low       = rat_index_low  * anchor_rats,
rats_high      = rat_index_high * anchor_rats
)
# 8. Write out for Tableau
write_csv(yearly_out, "output/yearly_rat_population.csv")
message("Wrote output/yearly_rat_population.csv (", nrow(yearly_out), " rows)")
View(yearly_out)
# ───────────────────────────────────────────────────────────
# Script: 042_rat_pop_boro.R
# Purpose: Allocate citywide rat-population estimates to boroughs by call volume
# Inputs:
#   • output/yearly_rat_population.csv
#   • data/processed/rats_ready.csv
# Outputs:
#   • output/boro_yearly_rat_population.csv
# Depends: readr, dplyr, lubridate
# ───────────────────────────────────────────────────────────
# 1. Load libraries
library(readr)
library(dplyr)
library(lubridate)
# 2. Load citywide annual rat-population estimates
annual <- read_csv("output/yearly_rat_population.csv", show_col_types = FALSE)
# 3. Load cleaned rat-call data and extract year
rats <- read_csv("data/processed/rats_ready.csv", show_col_types = FALSE) %>%
mutate(year = year(ymd_hms(Created))) %>%
filter(year <= 2024, !is.na(Borough))
# 4. Sum calls by borough and year
calls_by_boro <- rats %>%
group_by(Borough, year) %>%
summarise(calls = n(), .groups = "drop")
# 5. Compute total calls per year
total_calls <- calls_by_boro %>%
group_by(year) %>%
summarise(calls_total = sum(calls), .groups = "drop")
# 6. Allocate rat counts (and CI) proportionally by call share
boro_rats <- calls_by_boro %>%
left_join(total_calls, by = "year") %>%
left_join(annual %>% select(year, estimated_rats, rats_low, rats_high),
by = "year") %>%
mutate(
rats_est      = estimated_rats * (calls / calls_total),
rats_est_low  = rats_low       * (calls / calls_total),
rats_est_high = rats_high      * (calls / calls_total)
) %>%
select(
Borough, year, calls, calls_total,
estimated_rats, rats_low, rats_high,
rats_est, rats_est_low, rats_est_high
)
# 7. Write out borough-level rat-population CSV
write_csv(boro_rats, "output/boro_yearly_rat_population.csv")
message("Wrote output/boro_yearly_rat_population.csv (", nrow(boro_rats), " rows)")
View(boro_rats)
# ───────────────────────────────────────────────────────────
# Script: 043_funnel.R
# Purpose: Compute funnel metrics (calls → inspections → violations) for Tableau
# Inputs:
#   • data/processed/rats_clean.csv
#   • data/raw/Rodent_Inspection_20250611.csv
# Outputs:
#   • output/rat_funnel.csv
# Depends: data.table, stringr, lubridate
# ───────────────────────────────────────────────────────────
# 1. Load libraries
library(data.table)
library(stringr)
library(lubridate)
# 2. Read calls and inspections
rats <- fread("data/processed/rats_clean.csv")
insp <- fread("data/raw/Rodent_Inspection_20250611.csv")
# 3. Normalize column names for both tables
setnames(rats, c("Unique Key","created_dt","BBL"),
c("call_id","created_dt","bbl"))
setnames(insp, c("BBL","INSPECTION_DATE","RESULT"),
c("bbl","inspection_date","result"))
# 4. Keep only needed columns
rats <- rats[, .(call_id = as.character(call_id),
bbl     = as.character(bbl),
call_dt = ymd_hms(created_dt))]
insp <- insp[, .(bbl     = as.character(bbl),
insp_dt = ymd(inspection_date),
result_lc = tolower(result))]
# 5. Flag each inspection row
insp[, inspected := TRUE]
insp[, violation := str_detect(result_lc, "failed|nov") &
!str_detect(result_lc, "no violation")]
# 6. Prepare calls for non-equi join (±30 days)
setkey(insp, bbl, insp_dt)
rats[, low  := call_dt - days(30)]
rats[, high := call_dt + days(30)]
# 7. Non-equi join: match each call to first inspection in ±30-day window
matched <- insp[rats,
on = .(bbl,
insp_dt >= low,
insp_dt <= high),
mult = "first",
nomatch = 0L]
# 8. Collapse flags per call
flags <- matched[, .(inspected = TRUE,
violation = any(violation)),
by = call_id]
funnel <- merge(rats[, .(call_id)], flags,
by = "call_id", all.x = TRUE)[
is.na(inspected),  inspected := FALSE
][is.na(violation), violation := FALSE]
# 9. Compute funnel counts and percentages
Counts <- data.table(
stage        = c("Calls", "Inspections", "Violations"),
n            = c(nrow(funnel),
sum(funnel$inspected),
sum(funnel$violation))
)
Counts[, pct_of_calls := round(n / n[stage == "Calls"] * 100, 1)]
# 10. Write funnel summary for Tableau
dir.create("output", showWarnings = FALSE)
fwrite(Counts, "output/rat_funnel.csv")
print(Counts)
# ——— Duplicate block (repeats same steps) —————————————————————
library(data.table)
library(stringr)
library(lubridate)
rats <- fread("data/processed/rats_clean.csv")
insp <- fread("data/raw/Rodent_Inspection_20250611.csv")
setnames(rats, c("Unique Key","created_dt","BBL"),
c("call_id","created_dt","bbl"))
setnames(insp, c("BBL","INSPECTION_DATE","RESULT"),
c("bbl","inspection_date","result"))
rats <- rats[, .(call_id = as.character(call_id),
bbl     = as.character(bbl),
call_dt = ymd_hms(created_dt))]
insp <- insp[, .(bbl     = as.character(bbl),
insp_dt = ymd(inspection_date),
result_lc = tolower(result))]
insp[, inspected := TRUE]
insp[, violation := str_detect(result_lc, "failed|nov") &
!str_detect(result_lc, "no violation")]
setkey(insp, bbl, insp_dt)
rats[, low  := call_dt - days(30)]
rats[, high := call_dt + days(30)]
matched <- insp[rats,
on = .(bbl,
insp_dt >= low,
insp_dt <= high),
mult = "first",
nomatch = 0L]
flags <- matched[, .(inspected = TRUE,
violation = any(violation)),
by = call_id]
funnel <- merge(rats[, .(call_id)], flags,
by = "call_id", all.x = TRUE)[
is.na(inspected),  inspected := FALSE
][is.na(violation), violation := FALSE]
Counts <- data.table(
stage        = c("Calls", "Inspections", "Violations"),
n            = c(nrow(funnel),
sum(funnel$inspected),
sum(funnel$violation))
)
Counts[, pct_of_calls := round(n / n[stage == "Calls"] * 100, 1)]
dir.create("output", showWarnings = FALSE)
fwrite(Counts, "output/rat_funnel.csv")
print(Counts)
# ───────────────────────────────────────────────────────────
# Script: 036_deciles.R
# Purpose:
#   1. Compute median call rates per income decile (per quarter)
#   2. Compute citywide median and mean call rates per quarter
#   3. Compute average + total calls per income decile × quarter
# Inputs:
#   • output/bg_calls_ACS_summary.csv
#   • output/rats_enriched.geojson
# Outputs:
#   • output/decile_call_rates_vfinal.csv
#   • output/citywide_call_rate_qtr.csv
#   • output/decile_call_counts.csv
# Depends: sf, dplyr, readr, lubridate
# ───────────────────────────────────────────────────────────
library(sf)
library(dplyr)
library(readr)
library(lubridate)
# — Paths & parameters -------------------------------------
rats_path  <- "output/rats_enriched.geojson"
bg_path    <- "output/bg_calls_ACS_summary.csv"
out_dir    <- "output"
n_quarters <- 58  # quarters from 2010–Q1 to 2025–Q2
# 1. Load and prep BG summary for deciles ---------------
bg <- read_csv(bg_path, show_col_types = FALSE) %>%
mutate(
GEOID                = as.character(GEOID),
income_decile        = ntile(med_income, 10),
rate_per_qtr_per_10k = rate_per_10k / n_quarters
) %>%
select(GEOID, income_decile, rate_per_qtr_per_10k, pop_tot)
# 2. Compute & write median rate per decile -------------
decile_rates <- bg %>%
group_by(income_decile) %>%
summarise(
median_rate_per_qtr = median(rate_per_qtr_per_10k, na.rm = TRUE),
.groups = "drop"
)
write_csv(decile_rates, file.path(out_dir, "decile_call_rates_vfinal.csv"))
# 3. Compute & write citywide summary -------------------
rats <- st_read(rats_path, quiet = TRUE) %>% st_drop_geometry() %>%
mutate(
created_dt = ymd_hms(created_dt),
quarter    = paste0(year(created_dt), "-Q", quarter(created_dt))
)
city_pop_10k <- sum(bg$pop_tot, na.rm = TRUE) / 1e4
city_qtr <- rats %>%
group_by(quarter) %>%
summarise(calls = n(), .groups = "drop") %>%
mutate(rate_per_10k = calls / city_pop_10k)
city_stats <- city_qtr %>%
summarise(
median_rate_per_qtr = median(rate_per_10k, na.rm = TRUE),
mean_rate_per_qtr   = mean(rate_per_10k,   na.rm = TRUE),
.groups = "drop"
)
write_csv(city_stats, file.path(out_dir, "citywide_call_rate_qtr.csv"))
# 4. Compute & write decile × quarter call counts -------
rats2 <- st_read(rats_path, quiet = TRUE) %>%
st_drop_geometry() %>%
mutate(
created_dt = ymd_hms(created_dt),
quarter    = paste0(year(created_dt), "-Q", quarter(created_dt))
) %>%
left_join(bg %>% select(GEOID, income_decile), by = "GEOID")
# 4. Compute & write decile × quarter call counts -------
rats2 <- st_read(rats_path, quiet = TRUE) %>%
st_drop_geometry() %>%
mutate(
# ensure GEOID is character to match bg$GEOID
GEOID      = as.character(GEOID),
created_dt = ymd_hms(created_dt),
quarter    = paste0(year(created_dt), "-Q", quarter(created_dt))
) %>%
left_join(bg %>% select(GEOID, income_decile), by = "GEOID")
decile_counts <- rats2 %>%
group_by(income_decile, quarter) %>%
summarise(calls = n(), .groups = "drop") %>%
group_by(income_decile) %>%
summarise(
avg_calls_per_quarter = mean(calls),
total_calls           = sum(calls),
.groups = "drop"
)
write_csv(decile_counts, file.path(out_dir, "decile_call_counts.csv"))
message("✅ Wrote decile rates, city stats, and decile call counts to ", out_dir)
View(decile_counts)
View(decile_rates)
View(delta_draws)
