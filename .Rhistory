"output/rats_enriched.geojson",
"output/income_scatter.csv",
"output/rat_with_bg_ACS_point.csv",
"output/bg_calls_ACS_summary.csv"
)
missing <- expected[!file.exists(expected)]
if (length(missing) > 0) {
stop("‚ùå  Missing outputs: ", paste(missing, collapse = ", "))
}
message("üéâ  All scripts ran successfully and outputs are in place!")
# 031_qhcr_model.R
# -----------------------------------------------------------------------------
# Build quarterly QHCR model for rat abundance by Community District (CD_ID)
# Steps:
# 1) Read enriched sightings (with CD_ID)
# 2) Assign each sighting to a quarter
# 3) Build capture histories (counts) by CD_ID √ó quarter
# 4) Fit Stan-based QHCR model
# 5) Export posterior summaries to CSV (with correct CD_ID mapping)
# -----------------------------------------------------------------------------
# 0) Libraries
library(sf)
library(dplyr)
library(tidyr)
library(lubridate)
library(rstan)
library(posterior)
library(readr)
library(stringr)
# 1) Load enriched rat sightings
rats_sf <- sf::st_read(
"output/rats_enriched.geojson",
quiet = TRUE
) %>%
st_drop_geometry()
# 2) Assign each sighting to a quarter
rats_q <- rats_sf %>%
mutate(
call_date = ymd_hms(created_dt),  # parse the actual date-time field
quarter   = floor_date(call_date, unit = "quarter")
) %>%
filter(!is.na(CD_ID))
# 3) Build capture history matrix
districts <- sort(unique(rats_q$CD_ID))
occasions <- sort(unique(rats_q$quarter))
cap_hist <- rats_q %>%
group_by(CD_ID, quarter) %>%
summarise(calls = n(), .groups = "drop") %>%
complete(CD_ID = districts, quarter = occasions, fill = list(calls = 0)) %>%
arrange(match(CD_ID, districts), quarter) %>%
pivot_wider(names_from = quarter, values_from = calls)
# Convert to matrix for Stan
count_matrix <- as.matrix(cap_hist[, as.character(occasions)])
# 4) Prepare data for Stan
total_districts <- length(districts)
total_occasions <- length(occasions)
stan_data <- list(
D = total_districts,
T = total_occasions,
y = count_matrix
)
# 5) Fit QHCR model  <-- keep comment header
stan_mod <- rstan::stan_model("stan/qhcr_model_upgraded.stan")   # <-- new path
fit <- rstan::sampling(
stan_mod,
data   = stan_data,
iter   = 2000,
chains = 4,
control = list(adapt_delta = 0.9, max_treedepth = 12)          # <-- helps avoid divergences
)
# 6) Extract posterior summaries and map back to actual CD_ID values
# 'lambda[d]' in Stan corresponds to districts[d]
post <- as_draws_df(fit)
lambda_summ <- post %>%
select(starts_with("lambda[")) %>%
summarise_draws(
mean,
~quantile(.x, c(0.025, 0.975))
) %>%
rename(
estimate = mean,
lower    = `2.5%`,
upper    = `97.5%`
) %>%
mutate(
# extract the Stan index from variable name (e.g., "lambda[3]")
idx = as.integer(str_extract(variable, "(?<=\\[)\\d+(?=\\])")),
CD_ID = districts[idx]
) %>%
select(CD_ID, estimate, lower, upper)
# 7) Export to CSV
write_csv(lambda_summ, "output/district_qhcr.csv")
message("‚úÖ QHCR model complete ‚Äì wrote output/district_qhcr.csv")
# 6) Extract posterior summaries and map back to actual CD_ID values
# 'lambda[d]' in Stan corresponds to districts[d]
post <- as_draws_df(fit)
lambda_summ <- post %>%
select(starts_with("lambda[")) %>%
summarise_draws(
mean,
~quantile(.x, c(0.025, 0.975))
) %>%
rename(
estimate = mean,
lower    = `2.5%`,
upper    = `97.5%`
) %>%
mutate(
# extract the Stan index from variable name (e.g., "lambda[3]")
idx = as.integer(str_extract(variable, "(?<=\\[)\\d+(?=\\])")),
CD_ID = districts[idx]
) %>%
select(CD_ID, estimate, lower, upper)
# 6b) Extract quarter effects for seasonality viz -----------------------------
delta_summ <- post %>%                     # 'post' came from as_draws_df(fit)
select(starts_with("delta[")) %>%
summarise_draws(mean,
~quantile(.x, c(0.025, 0.975))) %>%
rename(
estimate = mean,
lower    = `2.5%`,
upper    = `97.5%`
) %>%
mutate(
idx     = as.integer(stringr::str_extract(variable, "(?<=\\[)\\d+(?=\\])")),
quarter = occasions[idx]               # occasions vector you built earlier
) %>%
select(quarter, estimate, lower, upper)
write_csv(delta_summ, "output/quarter_effects.csv")
# 7) Export to CSV
write_csv(lambda_summ, "output/district_qhcr.csv")
message("‚úÖ QHCR model complete ‚Äì wrote output/district_qhcr._v2csv")
# 7) Export to CSV
write_csv(lambda_summ, "output/district_qhcr_v2.csv")
message("‚úÖ QHCR model complete ‚Äì wrote output/district_qhcr._v2csv")
# Aggregate enriched 311 rat sightings to daily counts
#
# Reads output/rats_enriched.geojson, groups by date, and writes
# output/daily_rats.csv for the seasonality heat-map.
library(dplyr)
library(lubridate)
library(sf)
library(readr)
# 1) Read enriched points
rats <- sf::st_read("output/rats_enriched.geojson", quiet = TRUE) %>%
sf::st_drop_geometry()
# 2) Aggregate to one row per date
daily_counts <- rats %>%
mutate(date = as_date(ymd_hms(created_dt))) %>%
group_by(date) %>%
summarise(calls = n(), .groups = "drop")
# 3) Write CSV for Tableau
write_csv(daily_counts, "output/daily_rats.csv")
message("‚úÖ Wrote daily counts to output/daily_rats.csv")
# Aggregate enriched 311 rat sightings to daily counts
#
# Reads output/rats_enriched.geojson, groups by date, and writes
# output/daily_rats.csv for the seasonality heat-map.
library(dplyr)
library(lubridate)
library(sf)
library(readr)
# 1) Read enriched points (with CD_ID)
rats <- sf::st_read("output/rats_enriched.geojson", quiet = TRUE) %>%
sf::st_drop_geometry()
# 2) Build daily counts by CD_ID
daily_cd <- rats %>%
mutate(date = as_date(ymd_hms(created_dt))) %>%
filter(!is.na(CD_ID)) %>%
group_by(CD_ID, date) %>%
summarise(calls = n(), .groups = "drop")
# 3) Write out
write_csv(daily_cd, "output/daily_rats_by_cd.csv")
message("‚úÖ Wrote per-CD daily counts to output/daily_rats_by_cd.csv")
# Aggregate enriched 311 rat sightings to daily counts
#
# Reads output/rats_enriched.geojson, groups by date, and writes
# output/daily_rats.csv for the seasonality heat-map.
library(dplyr)
library(lubridate)
library(sf)
library(readr)
rats <- sf::st_read("output/rats_enriched.geojson", quiet=TRUE) %>%
st_drop_geometry() %>%
filter(!is.na(CD_ID)) %>%
mutate(quarter = floor_date(ymd_hms(created_dt), unit="quarter"))
quarterly_cd <- rats %>%
group_by(CD_ID, quarter) %>%
summarise(calls = n(), .groups="drop")
write_csv(quarterly_cd, "output/quarterly_rats_by_cd.csv")
message("‚úÖ Wrote per-CD quarterly counts to output/quarterly_rats_by_cd.csv")
View(rats_ready)
# Goal: turn the *raw* NYC 311 rat-related CSV into rats_clean. From which we can derive
#       other dataframes into smaller workable csvs
#
# The workflow, in plain English:
#   0. Make sure required packages are installed, then load them.
#   1. Point to the raw CSV (downloaded separately).
#   2. Pull a 5 000-row preview so you can eyeball column names & sample data.
#   3. Read *all* rows, parse dates, add QA/QC flags, classify event types.
#   4. Stick to the columns we actually need and filter to rat/rodent keywords.
#   5. Save the cleaned file for downstream use.
# -----------------------------------------------------------------------------
## 0) Install + load packages ----
#    Run `install.packages()` the *very first* time, then comment it out ‚úÇÔ∏è.
#    Keeping it here (commented) reminds newcomers what‚Äôs needed.
install.packages(c("vroom", "dplyr", "stringr", "lubridate", "readr", "rstudioapi"))
library(vroom)      # Fast CSV reader (multi-threaded)
library(dplyr)      # Data wrangling verbs (filter/ mutate / summarise ‚Ä¶)
library(stringr)    # Regex helpers that read like English
library(lubridate)  # Date-time parsing without tears
library(readr)      # write_csv() for output
## 1) Raw data path ----
#    Hard-coded for now; you might parameterise via config later.
data_path <- "data/raw/311_Service_Requests_from_2010_to_Present_20250526.csv"
## 2) Quick 5k-row peek ----
#    Safety check: confirms schema before loading millions of rows.
rats_5k_sample <- vroom(
file       = data_path,
n_max      = 5000,
col_select = c(
"Unique Key", "Created Date", "Closed Date", "Status",
"Resolution Description", "Location Type",
"Incident Zip", "Borough", "BBL",
"Latitude", "Longitude", "Descriptor"
)
)
print(head(rats_5k_sample, 5))
print(names(rats_5k_sample))
## 3) Full ingest + cleaning ----
#    We‚Äôll parse ~10M rows, so be mindful of memory on üçé/Windows laptops.
# a) Fixed reference dates
sentinel_date <- as_datetime("2010-01-01 00:00:00")  # anything closed *before* 2010 looks wrong
snapshot_date <- as.Date("2025-05-26")               # pretend ‚Äútoday‚Äù when ticket is still open
# b) Read the data ‚Äì narrow to columns we care about & set explicit types
rats_clean <- vroom(
file       = data_path,
col_select = c(
"Unique Key", "Created Date", "Closed Date", "Status",
"Resolution Description", "Location Type",
"Incident Zip", "Borough", "BBL",
"Latitude", "Longitude", "Descriptor"
),
col_types = cols(
`Unique Key`             = col_character(),
`Created Date`           = col_character(),
`Closed Date`            = col_character(),
Status                   = col_character(),
`Resolution Description` = col_character(),
`Location Type`          = col_character(),
`Incident Zip`           = col_character(),
Borough                  = col_character(),
BBL                      = col_character(),
Latitude                 = col_double(),
Longitude                = col_double(),
Descriptor               = col_character()
)
) %>%
mutate(
# Parse raw strings to POSIXct ‚Äì lubridate guesses the format.
created_dt    = mdy_hms(`Created Date`),
closed_dt_raw = mdy_hms(`Closed Date`),
# Flag suspicious closes (before NYC started publishing 311: 2010-01-01)
bad_close     = closed_dt_raw < sentinel_date,
# If closed date is bogus, treat as NA (ticket still open)
closed_dt     = if_else(bad_close, NA_POSIXct_, closed_dt_raw),
# Compute *days open* ‚Äì if closed use real diff, else diff to snapshot_date
days_open     = as.numeric(
if_else(
!is.na(closed_dt),
difftime(closed_dt, created_dt, units = "days"),
difftime(snapshot_date, as.Date(created_dt), units = "days")
),
units = "days"
),
# Flag tickets that have been open > 1 year (possible data issue)
stale_open    = days_open > 365,
# Categorise the *kind* of rat evidence ‚Äì may help in modelling later
event_type = case_when(
str_detect(Descriptor, regex("sighting",   ignore_case = TRUE)) ~ "direct",  # saw a rat
str_detect(Descriptor, regex("dropp|burrow", ignore_case = TRUE)) ~ "sign",   # found droppings, nests
TRUE                                                             ~ "other"
)
) %>%
# Keep only useful columns so downstream files stay slim
select(
`Unique Key`, created_dt, closed_dt, days_open,
bad_close, stale_open, event_type,
Status, `Resolution Description`,
`Location Type`, `Incident Zip`, Borough, BBL,
Latitude, Longitude, Descriptor
) %>%
# Focus on rows that look rodent-related (broad regex net)
filter(
str_starts(Descriptor, regex("Rat|Rodent|Mouse", ignore_case = TRUE)) |
str_detect(Descriptor, regex("dropp|burrow|bait|nest|runway|gnaw", ignore_case = TRUE))
)
## 4) Quick sanity report ----
message(
"‚úÖ Clean & flagged: ", nrow(rats_clean), " rows; ",
sum(rats_clean$bad_close), " bad_close; ",
sum(rats_clean$stale_open), " stale_open"
)
glimpse(rats_clean)
## 5) Write cleaned data to disk ----
write_csv(
rats_clean,
"data/processed/rats_clean.csv"
)
message("‚úèÔ∏è Wrote rats_clean.csv to data/processed/")
install.packages(c("vroom", "dplyr", "stringr", "lubridate", "readr", "rstudioapi"))
View(rats_clean)
gc()
# Master driver that runs the entire rat‚Äëdata pipeline **in order** and checks
# that every expected artefact is created. Handy for fresh environments or CI.
#
# Outline
# 0. (Optional) setwd() to the repo root.
# 1. Define the ordered vector of R scripts to source.
# 2. Loop through and `source()` each, echoing output.
# 3. Assert that the key CSV / GeoJSON files exist.
# ---------------------------------------------------------------------------
## 0) Working directory -------------------------------------------------------
# Uncomment and edit if you intend to run this outside the project root.
# setwd("C:/path/to/nyc-rat-abundance")
## 1) Scripts to run (ordered) -------------------------------------------------
scripts <- c(
"scripts/000_setup_cmdstanr.R",
"scripts/011_data_prep.R",
"scripts/012_data_prep_derivation.R",  # makes rats_ready + borough_rates
"scripts/021_acs_fetch.R",             # BBL‚Äëkeyed ACS lookup
"scripts/022_spatial_join.R",          # adds PLUTO & ACS to rat points
"scripts/023_income_scatter.R",        # tract‚Äëlevel scatter data
"scripts/024_bg_acs_join.R"            # BG‚Äëlevel join & summary
)
## 2) Run each script, stop on error -----------------------------------------
for (s in scripts) {
message("üîÑ  Running ", s, " ‚Ä¶")
source(s, echo = TRUE)
}
## 3) Verify outputs exist -----------------------------------------------------
expected <- c(
"data/processed/rats_ready.csv",
"data/processed/borough_rates.csv",
"data/processed/ACS.csv",
"output/rats_enriched.geojson",
"output/income_scatter.csv",
"output/rat_with_bg_ACS_point.csv",
"output/bg_calls_ACS_summary.csv"
)
missing <- expected[!file.exists(expected)]
if (length(missing) > 0) {
stop("‚ùå  Missing outputs: ", paste(missing, collapse = ", "))
}
message("üéâ  All scripts ran successfully and outputs are in place!")
install.packages(c("vroom", "dplyr", "stringr", "lubridate", "readr", "rstudioapi"))
# Aggregate enriched 311 rat sightings to daily counts
#
# Reads output/rats_enriched.geojson, groups by date, and writes
# output/daily_rats.csv for the seasonality heat-map.
library(dplyr)
library(lubridate)
library(sf)
library(readr)
# 1) Read enriched points (with CD_ID)
rats <- sf::st_read("output/rats_enriched.geojson", quiet = TRUE) %>%
sf::st_drop_geometry()
# 2) Build daily counts by CD_ID
daily_cd <- rats %>%
mutate(date = as_date(ymd_hms(created_dt))) %>%
filter(!is.na(CD_ID)) %>%
group_by(CD_ID, date) %>%
summarise(calls = n(), .groups = "drop")
# 3) Write out
write_csv(daily_cd, "output/daily_rats_by_cd.csv")
message("‚úÖ Wrote per-CD daily counts to output/daily_rats_by_cd.csv")
View(daily_cd)
View(rats_enriched)
View(income_scatter_df)
# Aggregate enriched 311 rat sightings to daily counts
#
# Reads output/rats_enriched.geojson, groups by date, and writes
# output/daily_rats.csv for the seasonality heat-map.
library(dplyr)
library(lubridate)
library(sf)
library(readr)
rats <- sf::st_read("output/rats_enriched.geojson", quiet=TRUE) %>%
st_drop_geometry() %>%
filter(!is.na(CD_ID)) %>%
mutate(quarter = floor_date(ymd_hms(created_dt), unit="quarter"))
quarterly_cd <- rats %>%
group_by(CD_ID, quarter) %>%
summarise(calls = n(), .groups="drop")
write_csv(quarterly_cd, "output/quarterly_rats_by_cd.csv")
message("‚úÖ Wrote per-CD quarterly counts to output/quarterly_rats_by_cd.csv")
library(dplyr)
library(readr)
# 1. Read in the data (adjust the path if needed)
df <- read_csv("data/raw/07152015-Check.csv")
# 2a. Find duplicated addresses
#    (replace 'Incident.Address' with the actual column name if different)
duplicate_addresses <- df %>%
filter(!is.na(Incident.Address), Incident.Address != "") %>%
group_by(Incident.Address) %>%
filter(n() > 1) %>%
ungroup()
library(dplyr)
library(readr)
# 1. Read in the data (adjust the path if needed)
df <- read_csv("data/raw/07152015-Check.csv")
# 2a. Find duplicated addresses
#    (replace 'Incident.Address' with the actual column name if different)
duplicate_addresses <- df %>%
filter(!is.na(Incident.Address), Incident.Address != "") %>%
group_by(Incident.Address) %>%
filter(n() > 1) %>%
ungroup()
View(df)
print(names(df))
#‚Äì‚Äì‚Äì Setup: install/load packages
pkgs <- c("dplyr", "readr", "tidyr", "stringr")
installed <- rownames(installed.packages())
for(p in pkgs) if(!(p %in% installed)) install.packages(p)
library(dplyr)
library(readr)
library(tidyr)
library(stringr)
#‚Äì‚Äì‚Äì 1. Read the data
df <- read_csv("data/raw/07152015-Check.csv", show_col_types = FALSE)
#‚Äì‚Äì‚Äì 2. Parse Location "(lat, lon)" into two numeric columns
df <- df %>%
mutate(
# remove parentheses
loc = str_remove_all(Location, "[\\(\\)]"),
# split on comma
Latitude  = as.numeric(str_trim(word(loc, 1, sep=","))),
Longitude = as.numeric(str_trim(word(loc, 2, sep=",")))
) %>%
select(-loc)  # drop helper
#‚Äì‚Äì‚Äì 3. Find duplicated Incident Addresses
addr_col <- "Incident Address"
duplicate_addresses <- df %>%
filter(!is.na(.data[[addr_col]]), .data[[addr_col]] != "") %>%
group_by(.data[[addr_col]]) %>%
filter(n() > 1) %>%
ungroup()
#‚Äì‚Äì‚Äì 4. Find duplicated coordinate pairs
duplicate_coords <- df %>%
filter(!is.na(Latitude), !is.na(Longitude)) %>%
group_by(Latitude, Longitude) %>%
filter(n() > 1) %>%
ungroup()
#‚Äì‚Äì‚Äì 5. Write results
write_csv(duplicate_addresses,   "duplicate_addresses.csv")
write_csv(duplicate_coords,      "duplicate_coordinates.csv")
#‚Äì‚Äì‚Äì 6. Print a quick summary
cat("‚Üí", n_distinct(duplicate_addresses[[addr_col]]),
"unique incident addresses have duplicates.\n")
cat("‚Üí", n_distinct(paste0(duplicate_coords$Latitude, ",", duplicate_coords$Longitude)),
"coordinate pairs have duplicates.\n")
View(duplicate_addresses)
View(duplicate_coords)
for(p in pkgs) if(!(p %in% installed)) install.packages(p)
library(dplyr)
library(readr)
library(tidyr)
library(stringr)
library(lubridate)
#‚Äì‚Äì‚Äì 1. Read the data
df <- read_csv("data/raw/07152015-Check.csv", show_col_types = FALSE)
#‚Äì‚Äì‚Äì 2. Parse the Created Date into POSIXct
#    Adjust the format inside mdy_hms() if your timestamps differ.
df <- df %>%
mutate(
Created_DT = mdy_hms(`Created Date`),
# 3. Parse Location "(lat, lon)" into two numeric columns
loc = str_remove_all(Location, "[\\(\\)]"),
Latitude  = as.numeric(str_trim(word(loc, 1, sep = ","))),
Longitude = as.numeric(str_trim(word(loc, 2, sep = ",")))
) %>%
select(-loc)
#‚Äì‚Äì‚Äì 4. Filter to 2015-07-15 00:00 through 2015-07-15 23:59
start_dt <- ymd_hms("2015-07-15 00:00:00")
end_dt   <- ymd_hms("2015-07-15 23:59:59")
df_day <- df %>%
filter(Created_DT >= start_dt, Created_DT <= end_dt)
#‚Äì‚Äì‚Äì 5. Find duplicated Incident Addresses
addr_col <- "Incident Address"
duplicate_addresses <- df_day %>%
filter(!is.na(.data[[addr_col]]), .data[[addr_col]] != "") %>%
group_by(.data[[addr_col]]) %>%
filter(n() > 1) %>%
ungroup()
#‚Äì‚Äì‚Äì 6. Find duplicated coordinate pairs
duplicate_coords <- df_day %>%
filter(!is.na(Latitude), !is.na(Longitude)) %>%
group_by(Latitude, Longitude) %>%
filter(n() > 1) %>%
ungroup()
#‚Äì‚Äì‚Äì 7. Write results
write_csv(duplicate_addresses, "duplicate_addresses.csv")
write_csv(duplicate_coords,    "duplicate_coordinates.csv")
#‚Äì‚Äì‚Äì 8. Print summary
cat("Date range:", format(start_dt), "to", format(end_dt), "\n")
cat("‚Üí", n_distinct(duplicate_addresses[[addr_col]]),
"unique incident addresses duplicated on 2015-07-15\n")
cat("‚Üí", n_distinct(paste0(duplicate_coords$Latitude, ",", duplicate_coords$Longitude)),
"coordinate pairs duplicated on 2015-07-15\n")
View(duplicate_coords)
View(duplicate_addresses)
View(duplicate_coords)
