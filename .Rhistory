pop_tot    = "B01003_001",  # total population
med_income = "B19013_001",  # median household income
pov_count  = "B17001_002"   # count below poverty
)
# 3) Download ACS block-group data (with geometry)
message("🔄 Downloading ACS block-group data for NYC…")
acs_bg <- get_acs(
geography = "block group",
variables = vars,
year      = 2019,
state     = "NY",
county    = c("Bronx", "Kings", "New York", "Queens", "Richmond"),
output    = "wide",
geometry  = TRUE
)
# 4) Read & project PLUTO parcels
message("🔄 Reading PLUTO parcels (MapPLUTO.shp)…")
bbl_sf <- st_read("data/raw/MapPLUTO.shp") %>%
st_transform(crs = st_crs(acs_bg))
# 4b) Repair any invalid geometries so centroids will compute
if (!all(st_is_valid(bbl_sf))) {
message("🔧 Repairing invalid geometries…")
bbl_sf <- sf::st_make_valid(bbl_sf)
}
# 5) Compute centroids of each lot
message("🔄 Computing centroids of each lot…")
bbl_centroids <- st_centroid(bbl_sf)
# 6) Spatial‐join each lot to its block‐group, keep GEOID + estimates
message("🔄 Joining lots to block groups…")
acs_by_bbl <- st_join(
bbl_centroids,
acs_bg %>% select(GEOID, ends_with("E")),
join = st_within
) %>%
st_drop_geometry() %>%
rename_with(~ str_remove(.x, "E$"), ends_with("E")) %>%
select(
BBL,        # tax-lot ID
GEOID,      # block-group ID
pop_tot,    # total population
med_income, # median household income
pov_count   # count below poverty
)
# 7) Write out the ACS-by-BBL table
message("🔄 Writing ACS×BBL table to data/raw/ACS.csv…")
write_csv(acs_by_bbl, "data/raw/ACS.csv")
# 8) Sanity check: sum each block-group’s pop_tot exactly once
total_pop <- acs_by_bbl %>%
distinct(GEOID, pop_tot) %>%
summarise(total_pop = sum(pop_tot, na.rm = TRUE)) %>%
pull(total_pop)
message("✅ Done! ACS table has ", nrow(acs_by_bbl),
" lots; sum of unique block-group pop_tot = ", total_pop)
# 0) Load required packages
library(sf)      # spatial data handling
library(dplyr)   # data wrangling
library(readr)   # CSV import
# 1) Read in your cleaned rats dataset
#    (make sure this matches where you saved it)
rats_clean <- read_csv("data/processed/rats_clean.csv")
# 2) Convert to sf point geometry
rat_sf <- st_as_sf(
rats_clean,
coords = c("Longitude", "Latitude"),  # x = lon, y = lat
crs    = 4326,                        # WGS84
remove = FALSE                        # keep raw lon/lat cols
)
View(acs_by_bbl)
View(acs_bg)
View(acs_by_bbl)
v19 <- load_variables(2019, "acs5", cache = TRUE)
v19 %>%
filter(str_detect(name, "B17001")) %>%
select(name, label)
View(v19)
View(v19)
write_csv(v19, "data/raw/gpt.csv")
View(v19)
# 1.5) Sanity Check if B17010_002 Exsits because NA result previously
v19 <- load_variables(2019, "acs5", cache = TRUE)
v19 %>%
filter(str_detect(name, "B17010_002")) %>%
select(name, label)
v19 <- load_variables(2019, "acs5", cache = TRUE)
v19 %>%
filter(str_detect(name, "B17010")) %>%
select(name, label)
# 0) Install & load needed packages (once; then comment out install.packages calls)
# install.packages(c("tidycensus", "sf", "dplyr", "readr", "stringr"))
library(tidycensus)   # to pull ACS data
library(sf)           # spatial tools for centroids & validity checks
library(dplyr)        # data wrangling
library(readr)        # CSV I/O
library(stringr)      # string helpers
# 1) Load your Census API key from ~/.Renviron
my_key <- Sys.getenv("CENSUS_API_KEY")
if (my_key == "") stop("🔑 CENSUS_API_KEY not found in ~/.Renviron")
census_api_key(my_key, install = FALSE)
# 1.5) Sanity Check if B17010_002 Exsits because NA result previously
v19 <- load_variables(2019, "acs5", cache = TRUE)
v19 %>%
filter(str_detect(name, "B17010")) %>%
select(name, label)
# 2) Define ACS variables (2019 5-year)
vars <- c(
pop_tot    = "B01003_001",  # total population
med_income = "B19013_001",  # median household income
pov_count  = "B17010_002"   # total population below poverty level
)
# 3) Download ACS block-group data (with geometry)
message("🔄 Downloading ACS block-group data for NYC…")
acs_bg <- get_acs(
geography = "block group",
variables = vars,
year      = 2019,
state     = "NY",
county    = c("Bronx", "Kings", "New York", "Queens", "Richmond"),
output    = "wide",
geometry  = TRUE
)
# 4) Read & project PLUTO parcels (MapPLUTO.shp)
message("🔄 Reading PLUTO parcels (MapPLUTO.shp)…")
bbl_sf <- st_read("data/raw/MapPLUTO.shp") %>%
st_transform(crs = st_crs(acs_bg))
# 4b) Repair any invalid geometries so centroids will compute
if (!all(st_is_valid(bbl_sf))) {
message("🔧 Repairing invalid geometries…")
bbl_sf <- sf::st_make_valid(bbl_sf)
}
# 5) Compute centroids of each lot
message("🔄 Computing centroids of each lot…")
bbl_centroids <- st_centroid(bbl_sf)
# 6) Spatial‐join each lot to its block‐group, keep GEOID + ACS estimates
message("🔄 Joining lots to block groups…")
acs_by_bbl <- st_join(
bbl_centroids,
acs_bg %>% select(GEOID, ends_with("E")),
join = st_within
) %>%
st_drop_geometry() %>%
# rename pop_totE → pop_tot, med_incomeE → med_income, pov_countE → pov_count
rename_with(~ str_remove(.x, "E$"), ends_with("E")) %>%
select(
BBL,        # tax-lot ID
GEOID,      # block-group ID
pop_tot,    # total population
med_income, # median household income
pov_count   # count below poverty level
)
# 7) Write out the ACS×BBL table
message("🔄 Writing ACS×BBL table to data/raw/ACS.csv…")
write_csv(acs_by_bbl, "data/raw/ACS.csv")
# 8) Sanity check: sum each block-group’s pop_tot exactly once
total_pop <- acs_by_bbl %>%
distinct(GEOID, pop_tot) %>%
summarise(total_pop = sum(pop_tot, na.rm = TRUE)) %>%
pull(total_pop)
message(
"✅ Done! ACS table has ",
nrow(acs_by_bbl),
" lots; sum of unique block-group pop_tot = ",
total_pop
)
View(acs_bg)
View(acs_by_bbl)
# 0) Load required packages
library(sf)      # spatial data handling
library(dplyr)   # data wrangling
library(readr)   # CSV import
# 1) Read in your cleaned rats dataset
#    (make sure this matches where you saved it)
rats_clean <- read_csv("data/processed/rats_clean.csv")
# 2) Convert to sf point geometry
rat_sf <- st_as_sf(
rats_clean,
coords = c("Longitude", "Latitude"),  # x = lon, y = lat
crs    = 4326,                        # WGS84
remove = FALSE                        # keep raw lon/lat cols
)
# 0) Load required packages
library(sf)      # spatial data handling
library(dplyr)   # data wrangling
library(readr)   # CSV import
# 1) Read in your cleaned rats dataset
rats_clean <- read_csv("data/processed/rats_clean.csv")
# 1b) Drop any rows with missing coordinates, since st_as_sf() won’t allow them
na_count <- rats_clean %>%
filter(is.na(Longitude) | is.na(Latitude)) %>%
nrow()
if (na_count > 0) {
message("⚠ Dropping ", na_count, " rows with missing Longitude/Latitude")
rats_clean <- rats_clean %>%
filter(!is.na(Longitude), !is.na(Latitude))
}
# 2) Convert to sf point geometry
rat_sf <- st_as_sf(
rats_clean,
coords = c("Longitude", "Latitude"),  # x = lon, y = lat
crs    = 4326,                        # WGS84
remove = FALSE                        # keep raw lon/lat cols
)
# 3) Read NYC tax-lot polygons (BBL shapefile)
bbl_sf <- st_read("data/raw/NYC_BBL_shapefile.shp") %>%
st_transform(crs = st_crs(rat_sf))   # ensure same CRS
# 0) Load required packages
library(sf)      # spatial data handling
library(dplyr)   # data wrangling
library(readr)   # CSV import
# 1) Read in your cleaned rats dataset
rats_clean <- read_csv("data/processed/rats_clean.csv")
# 1b) Drop rows with missing coords
missing_n <- rats_clean %>%
filter(is.na(Longitude) | is.na(Latitude)) %>%
nrow()
if (missing_n > 0) {
message("⚠ Dropping ", missing_n, " rows with missing Longitude/Latitude")
rats_clean <- rats_clean %>%
filter(!is.na(Longitude), !is.na(Latitude))
}
# 2) Convert to sf point geometry
rat_sf <- st_as_sf(
rats_clean,
coords = c("Longitude", "Latitude"),
crs    = 4326,
remove = FALSE
)
# 3) Read NYC tax-lot polygons (PLUTO shapefile)
bbl_sf <- st_read("data/raw/MapPLUTO.shp") %>%
st_transform(crs = st_crs(rat_sf))
# 4) Spatial-join: stamp each sighting with its BBL
rat_bbl <- st_join(
rat_sf,
bbl_sf %>% select(BBL),
join = st_within,
left = TRUE
)
# 0) Load required packages
library(sf)      # spatial data handling
library(dplyr)   # data wrangling
library(readr)   # CSV import
# 1) Read in your cleaned rats dataset
rats_clean <- read_csv("data/processed/rats_clean.csv")
# 1b) Drop any rows with missing coords
missing_n <- rats_clean %>%
filter(is.na(Longitude) | is.na(Latitude)) %>%
nrow()
if (missing_n > 0) {
message("⚠ Dropping ", missing_n, " rows with missing Longitude/Latitude")
rats_clean <- rats_clean %>%
filter(!is.na(Longitude), !is.na(Latitude))
}
# 2) Convert to sf point geometry
rat_sf <- st_as_sf(
rats_clean,
coords = c("Longitude", "Latitude"),
crs    = 4326,
remove = FALSE
)
# 3) Read PLUTO parcels and repair invalid geometries
bbl_sf <- st_read("data/raw/MapPLUTO.shp") %>%
st_transform(crs = st_crs(rat_sf))
if (!all(st_is_valid(bbl_sf))) {
message("🔧 Repairing invalid PLUTO geometries…")
bbl_sf <- sf::st_make_valid(bbl_sf)
}
# 4) Spatial-join: stamp each sighting with its BBL
rat_bbl <- st_join(
rat_sf,
bbl_sf %>% select(BBL),
join = st_within,
left = TRUE
)
# 5) Read in PLUTO & ACS tables (keyed by BBL)
pluto <- read_csv("data/raw/PLUTO.csv")
# 5) Read in PLUTO parcels (MapPLUTO.shp) for attributes
message("🔄 Reading PLUTO shapefile for attributes…")
pluto_sf <- st_read("data/raw/MapPLUTO.shp")
# 5b) Drop geometry to get the flat attribute table
pluto <- pluto_sf %>%
st_drop_geometry()
# 6) Read in ACS table (keyed by BBL)
message("🔄 Reading ACS×BBL CSV…")
acs <- read_csv("data/raw/ACS.csv")
# 7) Attribute-join covariates to each sighting
rat_enriched <- rat_bbl %>%
left_join(pluto, by = "BBL") %>%
left_join(acs,   by = "BBL")
View(rat_bbl)
rlang::last_trace()
View(rat_bbl)
View(rats_clean)
View(rat_bbl)
# 0) Load required packages
library(sf)      # spatial data handling
library(dplyr)   # data wrangling
library(readr)   # CSV import
# 1) Read in your cleaned rats dataset
rats_clean <- read_csv("data/processed/rats_clean.csv")
# 1b) Drop any rows with missing coords
missing_n <- rats_clean %>%
filter(is.na(Longitude) | is.na(Latitude)) %>%
nrow()
if (missing_n > 0) {
message("⚠ Dropping ", missing_n, " rows with missing Longitude/Latitude")
rats_clean <- rats_clean %>%
filter(!is.na(Longitude), !is.na(Latitude))
}
# 2) Convert to sf point geometry
rat_sf <- st_as_sf(
rats_clean,
coords = c("Longitude", "Latitude"),
crs    = 4326,
remove = FALSE
)
# 3) Read PLUTO parcels and repair invalid geometries
bbl_sf <- st_read("data/raw/MapPLUTO.shp") %>%
st_transform(crs = st_crs(rat_sf))
if (!all(st_is_valid(bbl_sf))) {
message("🔧 Repairing invalid PLUTO geometries…")
bbl_sf <- sf::st_make_valid(bbl_sf)
}
# 4) Spatial‐join, pulling BBL in under a temp name to avoid collision
rat_bbl <- st_join(
rat_sf,
bbl_sf %>% select(join_BBL = BBL),
join = st_within,
left = TRUE
) %>%
rename(BBL = join_BBL)
rlang::last_trace()
View(rat_bbl)
View(rats_clean)
pluto <- st_read("data/raw/MapPLUTO.shp") %>%
st_drop_geometry()
View(api)
view(census_api_key())
# 0) Load required packages
library(sf)      # spatial data handling
library(dplyr)   # data wrangling
library(readr)   # CSV import
# 1) Read in your cleaned rats dataset (kept intact)
rats_clean <- read_csv("data/processed/rats_clean.csv")
# 2) Create a join‐ready copy and drop its old BBL
rats_clean_join <- rats_clean %>%
select(-BBL)
# 2b) Drop any rows in the join copy with missing coords
missing_n <- rats_clean_join %>%
filter(is.na(Longitude) | is.na(Latitude)) %>%
nrow()
if (missing_n > 0) {
message("⚠ Dropping ", missing_n, " rows with missing Longitude/Latitude")
rats_clean_join <- rats_clean_join %>%
filter(!is.na(Longitude), !is.na(Latitude))
}
# 3) Convert the join copy to sf point geometry
rat_sf <- st_as_sf(
rats_clean_join,
coords = c("Longitude", "Latitude"),
crs    = 4326,
remove = FALSE
)
# 4) Read PLUTO parcels and repair invalid geometries
bbl_sf <- st_read("data/raw/MapPLUTO.shp") %>%
st_transform(crs = st_crs(rat_sf))
if (!all(st_is_valid(bbl_sf))) {
message("🔧 Repairing invalid PLUTO geometries…")
bbl_sf <- sf::st_make_valid(bbl_sf)
}
# 5) Spatial‐join: bring in the parcel BBL under a temp name
rat_bbl <- st_join(
rat_sf,
bbl_sf %>% select(join_BBL = BBL),
join = st_within,
left = TRUE
) %>%
rename(BBL = join_BBL)
# 6) Read PLUTO attributes (directly from shapefile) and drop geometry
message("🔄 Reading PLUTO shapefile for attributes…")
pluto <- st_read("data/raw/MapPLUTO.shp") %>%
st_drop_geometry()
# 7) Read in ACS table (keyed by BBL)
message("🔄 Reading ACS×BBL CSV…")
acs <- read_csv("data/raw/ACS.csv")
# 8) Attribute‐join covariates to each sighting
rat_enriched <- rat_bbl %>%
left_join(pluto, by = "BBL") %>%
left_join(acs,   by = "BBL")
# 9) Ensure outputs folder exists
if (!dir.exists("outputs")) dir.create("outputs", recursive = TRUE)
# 10) Write out enriched GeoJSON
st_write(
rat_enriched,
"outputs/rat_clean.geojson",
driver     = "GeoJSON",
delete_dsn = TRUE
)
message("✅ Spatial join complete: outputs/rat_clean.geojson written")
# 000_setup_cmdstanr.R
#
# This script gets **CmdStanR** up and running on Windows so you can compile
# and run Stan models from R.
#
# What happens, step by step:
#   1. Install the *R package* **cmdstanr** (one-time).
#   2. Make sure R can find a C++ compiler (comes with RTools).
#   3. Download & build the CmdStan C++ source (only first time; ~1 GB).
#   4. Compile the tiny built-in Bernoulli example model.
#   5. Run a quick 8-data-point MCMC sample to prove everything works.
#
# Re-running is safe: most steps notice they’re already done and skip.
# -----------------------------------------------------------------------------
# 1) Install cmdstanr if it’s missing ------------------------------------------
if (!requireNamespace("cmdstanr", quietly = TRUE)) {
install.packages(
"cmdstanr",
repos = c("https://mc-stan.org/r-packages/", getOption("repos"))
)
} else {
message("cmdstanr already installed — nice!")
}
# 2) Load the package so we can call its helpers --------------------------------
library(cmdstanr)
# 3) Check that the C++ toolchain (gcc/make) is visible to R --------------------
#    `fix = TRUE` tries to patch common PATH problems for you.
cmdstanr::check_cmdstan_toolchain(fix = TRUE)
# 000_setup_cmdstanr.R
#
# This script gets **CmdStanR** up and running on Windows so you can compile
# and run Stan models from R.
#
# What happens, step by step:
#   1. Install the *R package* **cmdstanr** (one-time).
#   2. Make sure R can find a C++ compiler (comes with RTools).
#   3. Download & build the CmdStan C++ source (only first time; ~1 GB).
#   4. Compile the tiny built-in Bernoulli example model.
#   5. Run a quick 8-data-point MCMC sample to prove everything works.
#
# Re-running is safe: most steps notice they’re already done and skip.
# -----------------------------------------------------------------------------
# 1) Install cmdstanr if it’s missing ------------------------------------------
if (!requireNamespace("cmdstanr", quietly = TRUE)) {
install.packages(
"cmdstanr",
repos = c("https://mc-stan.org/r-packages/", getOption("repos"))
)
} else {
message("cmdstanr already installed — nice!")
}
# 2) Load the package so we can call its helpers --------------------------------
library(cmdstanr)
# 3) Check that the C++ toolchain (gcc/make) is visible to R --------------------
#    `fix = TRUE` tries to patch common PATH problems for you.
cmdstanr::check_cmdstan_toolchain(fix = TRUE)
file.edit("~/.Renviron")
# 000_setup_cmdstanr.R
#
# This script gets **CmdStanR** up and running on Windows so you can compile
# and run Stan models from R.
#
# What happens, step by step:
#   1. Install the *R package* **cmdstanr** (one-time).
#   2. Make sure R can find a C++ compiler (comes with RTools).
#   3. Download & build the CmdStan C++ source (only first time; ~1 GB).
#   4. Compile the tiny built-in Bernoulli example model.
#   5. Run a quick 8-data-point MCMC sample to prove everything works.
#
# Re-running is safe: most steps notice they’re already done and skip.
# -----------------------------------------------------------------------------
# 1) Install cmdstanr if it’s missing ------------------------------------------
if (!requireNamespace("cmdstanr", quietly = TRUE)) {
install.packages(
"cmdstanr",
repos = c("https://mc-stan.org/r-packages/", getOption("repos"))
)
} else {
message("cmdstanr already installed — nice!")
}
# 2) Load the package so we can call its helpers --------------------------------
library(cmdstanr)
# 3) Check that the C++ toolchain (gcc/make) is visible to R --------------------
#    `fix = TRUE` tries to patch common PATH problems for you.
cmdstanr::check_cmdstan_toolchain(fix = TRUE)
# 000_setup_cmdstanr.R
#
# This script gets **CmdStanR** up and running on Windows so you can compile
# and run Stan models from R.
#
# What happens, step by step:
#   1. Install the *R package* **cmdstanr** (one-time).
#   2. Make sure R can find a C++ compiler (comes with RTools).
#   3. Download & build the CmdStan C++ source (only first time; ~1 GB).
#   4. Compile the tiny built-in Bernoulli example model.
#   5. Run a quick 8-data-point MCMC sample to prove everything works.
#
# Re-running is safe: most steps notice they’re already done and skip.
# -----------------------------------------------------------------------------
# 1) Install cmdstanr if it’s missing ------------------------------------------
if (!requireNamespace("cmdstanr", quietly = TRUE)) {
install.packages(
"cmdstanr",
repos = c("https://mc-stan.org/r-packages/", getOption("repos"))
)
} else {
message("cmdstanr already installed — nice!")
}
# 2) Load the package so we can call its helpers --------------------------------
library(cmdstanr)
# 3) Check that the C++ toolchain (gcc/make) is visible to R --------------------
#    `fix = TRUE` tries to patch common PATH problems for you.
cmdstanr::check_cmdstan_toolchain(fix = TRUE)
Sys.getenv("RTOOLS44_HOME")
cmdstanr::check_cmdstan_toolchain()
