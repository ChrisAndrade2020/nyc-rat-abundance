T = length(occasions),
y = count_matrix
)
# 5) Fit QHCR model
stan_mod <- rstan::stan_model("stan/qhcr_model.stan")
fit <- rstan::sampling(
object = stan_mod,
data   = stan_data,
iter   = 2000,
chains = 4,
cores  = parallel::detectCores()
)
# 6) Extract posterior summaries and map back to actual CD_ID values
# 'lambda[d]' in Stan corresponds to districts[d]
post <- as_draws_df(fit)
lambda_summ <- post %>%
select(starts_with("lambda[")) %>%
summarise_draws(
mean,
~quantile(.x, c(0.025, 0.975))
) %>%
rename(
estimate = mean,
lower    = `2.5%`,
upper    = `97.5%`
) %>%
# extract the Stan index, then map to the real CD_ID
mutate(
idx = as.integer(str_extract(variable, "(?<=\[)\d+(?=\])")),
# 031_qhcr_model.R
# -----------------------------------------------------------------------------
# Build quarterly QHCR model for rat abundance by Community District (CD_ID)
# Steps:
# 1) Read enriched sightings (with CD_ID)
# 2) Assign each sighting to a quarter
# 3) Build capture histories (counts) by CD_ID × quarter
# 4) Fit Stan-based QHCR model
# 5) Export posterior summaries to CSV (with correct CD_ID mapping)
# -----------------------------------------------------------------------------
# 0) Libraries
library(sf)
library(dplyr)
library(tidyr)
library(lubridate)
library(rstan)
library(posterior)
library(readr)
library(stringr)
# 1) Load enriched rat sightings
rats_sf <- sf::st_read(
"output/rats_enriched.geojson",
quiet = TRUE
) %>%
st_drop_geometry()
# 2) Assign each sighting to a quarter
rats_q <- rats_sf %>%
mutate(
call_date = ymd_hms(created_dt),  # parse the actual date-time field
quarter   = floor_date(call_date, unit = "quarter")
) %>%
filter(!is.na(CD_ID))
# 3) Build capture history matrix
districts <- sort(unique(rats_q$CD_ID))
occasions <- sort(unique(rats_q$quarter))
cap_hist <- rats_q %>%
group_by(CD_ID, quarter) %>%
summarise(calls = n(), .groups = "drop") %>%
complete(CD_ID = districts, quarter = occasions, fill = list(calls = 0)) %>%
arrange(match(CD_ID, districts), quarter) %>%
pivot_wider(names_from = quarter, values_from = calls)
# Convert to matrix for Stan
count_matrix <- as.matrix(cap_hist[, as.character(occasions)])
# 4) Prepare data for Stan
total_districts <- length(districts)
total_occasions <- length(occasions)
stan_data <- list(
D = total_districts,
T = total_occasions,
y = count_matrix
)
# 5) Fit QHCR model
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
stan_mod <- rstan::stan_model("stan/qhcr_model.stan")
fit <- rstan::sampling(
stan_mod,
data   = stan_data,
iter   = 2000,
chains = 4
)
# 6) Extract posterior summaries and map back to actual CD_ID values
# 'lambda[d]' in Stan corresponds to districts[d]
post <- as_draws_df(fit)
lambda_summ <- post %>%
select(starts_with("lambda[")) %>%
summarise_draws(
mean,
~quantile(.x, c(0.025, 0.975))
) %>%
rename(
estimate = mean,
lower    = `2.5%`,
upper    = `97.5%`
) %>%
mutate(
# extract the Stan index from variable name (e.g., "lambda[3]")
idx = as.integer(str_extract(variable, "(?<=\\[)\\d+(?=\\])")),
CD_ID = districts[idx]
) %>%
select(CD_ID, estimate, lower, upper)
# 7) Export to CSV
write_csv(lambda_summ, "output/district_qhcr.csv")
message("✅ QHCR model complete – wrote output/district_qhcr.csv")
readr::read_csv("output/district_qhcr.csv") %>% head()
# 031_qhcr_model.R
# -----------------------------------------------------------------------------
# Build quarterly QHCR model for rat abundance by Community District (CD_ID)
# Steps:
# 1) Read enriched sightings (with CD_ID)
# 2) Assign each sighting to a quarter
# 3) Build capture histories (counts) by CD_ID × quarter
# 4) Fit Stan-based QHCR model
# 5) Export posterior summaries to CSV (with correct CD_ID mapping)
# -----------------------------------------------------------------------------
# 0) Libraries
library(sf)
library(dplyr)
library(tidyr)
library(lubridate)
library(rstan)
library(posterior)
library(readr)
library(stringr)
# 1) Load enriched rat sightings
rats_sf <- sf::st_read(
"output/rats_enriched.geojson",
quiet = TRUE
) %>%
st_drop_geometry()
# 2) Assign each sighting to a quarter
rats_q <- rats_sf %>%
mutate(
call_date = ymd_hms(created_dt),  # parse the actual date-time field
quarter   = floor_date(call_date, unit = "quarter")
) %>%
filter(!is.na(CD_ID))
# 3) Build capture history matrix
districts <- sort(unique(rats_q$CD_ID))
occasions <- sort(unique(rats_q$quarter))
cap_hist <- rats_q %>%
group_by(CD_ID, quarter) %>%
summarise(calls = n(), .groups = "drop") %>%
complete(CD_ID = districts, quarter = occasions, fill = list(calls = 0)) %>%
arrange(match(CD_ID, districts), quarter) %>%
pivot_wider(names_from = quarter, values_from = calls)
# Convert to matrix for Stan
count_matrix <- as.matrix(cap_hist[, as.character(occasions)])
# 4) Prepare data for Stan
total_districts <- length(districts)
total_occasions <- length(occasions)
stan_data <- list(
D = total_districts,
T = total_occasions,
y = count_matrix
)
# 5) Fit QHCR model
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
stan_mod <- rstan::stan_model("stan/qhcr_model.stan")
fit <- rstan::sampling(
stan_mod,
data   = stan_data,
iter   = 2000,
chains = 4
)
# 6) Extract posterior summaries and map back to actual CD_ID values
# 'lambda[d]' in Stan corresponds to districts[d]
post <- as_draws_df(fit)
lambda_summ <- post %>%
select(starts_with("lambda[")) %>%
summarise_draws(
mean,
~quantile(.x, c(0.025, 0.975))
) %>%
rename(
estimate = mean,
lower    = `2.5%`,
upper    = `97.5%`
) %>%
mutate(
# extract the Stan index from variable name (e.g., "lambda[3]")
idx = as.integer(str_extract(variable, "(?<=\\[)\\d+(?=\\])")),
CD_ID = districts[idx]
) %>%
select(CD_ID, estimate, lower, upper)
# 7) Export to CSV
write_csv(lambda_summ, "output/district_qhcr.csv")
message("✅ QHCR model complete – wrote output/district_qhcr.csv")
# Master driver that runs the entire rat‑data pipeline **in order** and checks
# that every expected artefact is created. Handy for fresh environments or CI.
#
# Outline
# 0. (Optional) setwd() to the repo root.
# 1. Define the ordered vector of R scripts to source.
# 2. Loop through and `source()` each, echoing output.
# 3. Assert that the key CSV / GeoJSON files exist.
# ---------------------------------------------------------------------------
## 0) Working directory -------------------------------------------------------
# Uncomment and edit if you intend to run this outside the project root.
# setwd("C:/path/to/nyc-rat-abundance")
## 1) Scripts to run (ordered) -------------------------------------------------
scripts <- c(
"scripts/000_setup_cmdstanr.R",
"scripts/011_data_prep.R",
"scripts/012_data_prep_derivation.R",  # makes rats_ready + borough_rates
"scripts/021_acs_fetch.R",             # BBL‑keyed ACS lookup
"scripts/022_spatial_join.R",          # adds PLUTO & ACS to rat points
"scripts/023_income_scatter.R",        # tract‑level scatter data
"scripts/024_bg_acs_join.R"            # BG‑level join & summary
)
## 2) Run each script, stop on error -----------------------------------------
for (s in scripts) {
message("🔄  Running ", s, " …")
source(s, echo = TRUE)
}
gc()
# Master driver that runs the entire rat‑data pipeline **in order** and checks
# that every expected artefact is created. Handy for fresh environments or CI.
#
# Outline
# 0. (Optional) setwd() to the repo root.
# 1. Define the ordered vector of R scripts to source.
# 2. Loop through and `source()` each, echoing output.
# 3. Assert that the key CSV / GeoJSON files exist.
# ---------------------------------------------------------------------------
## 0) Working directory -------------------------------------------------------
# Uncomment and edit if you intend to run this outside the project root.
# setwd("C:/path/to/nyc-rat-abundance")
## 1) Scripts to run (ordered) -------------------------------------------------
scripts <- c(
"scripts/000_setup_cmdstanr.R",
"scripts/011_data_prep.R",
"scripts/012_data_prep_derivation.R",  # makes rats_ready + borough_rates
"scripts/021_acs_fetch.R",             # BBL‑keyed ACS lookup
"scripts/022_spatial_join.R",          # adds PLUTO & ACS to rat points
"scripts/023_income_scatter.R",        # tract‑level scatter data
"scripts/024_bg_acs_join.R"            # BG‑level join & summary
)
## 2) Run each script, stop on error -----------------------------------------
for (s in scripts) {
message("🔄  Running ", s, " …")
source(s, echo = TRUE)
}
install.packages(c("vroom", "dplyr", "stringr", "lubridate", "readr", "rstudioapi"))
# Master driver that runs the entire rat‑data pipeline **in order** and checks
# that every expected artefact is created. Handy for fresh environments or CI.
#
# Outline
# 0. (Optional) setwd() to the repo root.
# 1. Define the ordered vector of R scripts to source.
# 2. Loop through and `source()` each, echoing output.
# 3. Assert that the key CSV / GeoJSON files exist.
# ---------------------------------------------------------------------------
## 0) Working directory -------------------------------------------------------
# Uncomment and edit if you intend to run this outside the project root.
# setwd("C:/path/to/nyc-rat-abundance")
## 1) Scripts to run (ordered) -------------------------------------------------
scripts <- c(
"scripts/000_setup_cmdstanr.R",
"scripts/011_data_prep.R",
"scripts/012_data_prep_derivation.R",  # makes rats_ready + borough_rates
"scripts/021_acs_fetch.R",             # BBL‑keyed ACS lookup
"scripts/022_spatial_join.R",          # adds PLUTO & ACS to rat points
"scripts/023_income_scatter.R",        # tract‑level scatter data
"scripts/024_bg_acs_join.R"            # BG‑level join & summary
)
## 2) Run each script, stop on error -----------------------------------------
for (s in scripts) {
message("🔄  Running ", s, " …")
source(s, echo = TRUE)
}
View(rat_sf)
View(rat_sf)
View(bbl_sf)
View(rat_sf)
print(names(cd_sf))
View(cd_sf)
gc()
# Master driver that runs the entire rat‑data pipeline **in order** and checks
# that every expected artefact is created. Handy for fresh environments or CI.
#
# Outline
# 0. (Optional) setwd() to the repo root.
# 1. Define the ordered vector of R scripts to source.
# 2. Loop through and `source()` each, echoing output.
# 3. Assert that the key CSV / GeoJSON files exist.
# ---------------------------------------------------------------------------
## 0) Working directory -------------------------------------------------------
# Uncomment and edit if you intend to run this outside the project root.
# setwd("C:/path/to/nyc-rat-abundance")
## 1) Scripts to run (ordered) -------------------------------------------------
scripts <- c(
"scripts/000_setup_cmdstanr.R",
"scripts/011_data_prep.R",
"scripts/012_data_prep_derivation.R",  # makes rats_ready + borough_rates
"scripts/021_acs_fetch.R",             # BBL‑keyed ACS lookup
"scripts/022_spatial_join.R",          # adds PLUTO & ACS to rat points
"scripts/023_income_scatter.R",        # tract‑level scatter data
"scripts/024_bg_acs_join.R"            # BG‑level join & summary
)
## 2) Run each script, stop on error -----------------------------------------
for (s in scripts) {
message("🔄  Running ", s, " …")
source(s, echo = TRUE)
}
## 3) Verify outputs exist -----------------------------------------------------
expected <- c(
"data/processed/rats_ready.csv",
"data/processed/borough_rates.csv",
"data/processed/ACS.csv",
"output/rats_enriched.geojson",
"output/income_scatter.csv",
"output/rat_with_bg_ACS_point.csv",
"output/bg_calls_ACS_summary.csv"
)
missing <- expected[!file.exists(expected)]
if (length(missing) > 0) {
stop("❌  Missing outputs: ", paste(missing, collapse = ", "))
}
message("🎉  All scripts ran successfully and outputs are in place!")
# 031_qhcr_model.R
# -----------------------------------------------------------------------------
# Build quarterly QHCR model for rat abundance by Community District (CD_ID)
# Steps:
# 1) Read enriched sightings (with CD_ID)
# 2) Assign each sighting to a quarter
# 3) Build capture histories (counts) by CD_ID × quarter
# 4) Fit Stan-based QHCR model
# 5) Export posterior summaries to CSV (with correct CD_ID mapping)
# -----------------------------------------------------------------------------
# 0) Libraries
library(sf)
library(dplyr)
library(tidyr)
library(lubridate)
library(rstan)
library(posterior)
library(readr)
library(stringr)
# 1) Load enriched rat sightings
rats_sf <- sf::st_read(
"output/rats_enriched.geojson",
quiet = TRUE
) %>%
st_drop_geometry()
# 2) Assign each sighting to a quarter
rats_q <- rats_sf %>%
mutate(
call_date = ymd_hms(created_dt),  # parse the actual date-time field
quarter   = floor_date(call_date, unit = "quarter")
) %>%
filter(!is.na(CD_ID))
# 3) Build capture history matrix
districts <- sort(unique(rats_q$CD_ID))
occasions <- sort(unique(rats_q$quarter))
cap_hist <- rats_q %>%
group_by(CD_ID, quarter) %>%
summarise(calls = n(), .groups = "drop") %>%
complete(CD_ID = districts, quarter = occasions, fill = list(calls = 0)) %>%
arrange(match(CD_ID, districts), quarter) %>%
pivot_wider(names_from = quarter, values_from = calls)
# Convert to matrix for Stan
count_matrix <- as.matrix(cap_hist[, as.character(occasions)])
# 4) Prepare data for Stan
total_districts <- length(districts)
total_occasions <- length(occasions)
stan_data <- list(
D = total_districts,
T = total_occasions,
y = count_matrix
)
# 5) Fit QHCR model  <-- keep comment header
stan_mod <- rstan::stan_model("stan/qhcr_model_upgraded.stan")   # <-- new path
fit <- rstan::sampling(
stan_mod,
data   = stan_data,
iter   = 2000,
chains = 4,
control = list(adapt_delta = 0.9, max_treedepth = 12)          # <-- helps avoid divergences
)
# 6) Extract posterior summaries and map back to actual CD_ID values
# 'lambda[d]' in Stan corresponds to districts[d]
post <- as_draws_df(fit)
lambda_summ <- post %>%
select(starts_with("lambda[")) %>%
summarise_draws(
mean,
~quantile(.x, c(0.025, 0.975))
) %>%
rename(
estimate = mean,
lower    = `2.5%`,
upper    = `97.5%`
) %>%
mutate(
# extract the Stan index from variable name (e.g., "lambda[3]")
idx = as.integer(str_extract(variable, "(?<=\\[)\\d+(?=\\])")),
CD_ID = districts[idx]
) %>%
select(CD_ID, estimate, lower, upper)
# 7) Export to CSV
write_csv(lambda_summ, "output/district_qhcr.csv")
message("✅ QHCR model complete – wrote output/district_qhcr.csv")
# 6) Extract posterior summaries and map back to actual CD_ID values
# 'lambda[d]' in Stan corresponds to districts[d]
post <- as_draws_df(fit)
lambda_summ <- post %>%
select(starts_with("lambda[")) %>%
summarise_draws(
mean,
~quantile(.x, c(0.025, 0.975))
) %>%
rename(
estimate = mean,
lower    = `2.5%`,
upper    = `97.5%`
) %>%
mutate(
# extract the Stan index from variable name (e.g., "lambda[3]")
idx = as.integer(str_extract(variable, "(?<=\\[)\\d+(?=\\])")),
CD_ID = districts[idx]
) %>%
select(CD_ID, estimate, lower, upper)
# 6b) Extract quarter effects for seasonality viz -----------------------------
delta_summ <- post %>%                     # 'post' came from as_draws_df(fit)
select(starts_with("delta[")) %>%
summarise_draws(mean,
~quantile(.x, c(0.025, 0.975))) %>%
rename(
estimate = mean,
lower    = `2.5%`,
upper    = `97.5%`
) %>%
mutate(
idx     = as.integer(stringr::str_extract(variable, "(?<=\\[)\\d+(?=\\])")),
quarter = occasions[idx]               # occasions vector you built earlier
) %>%
select(quarter, estimate, lower, upper)
write_csv(delta_summ, "output/quarter_effects.csv")
# 7) Export to CSV
write_csv(lambda_summ, "output/district_qhcr.csv")
message("✅ QHCR model complete – wrote output/district_qhcr._v2csv")
# 7) Export to CSV
write_csv(lambda_summ, "output/district_qhcr_v2.csv")
message("✅ QHCR model complete – wrote output/district_qhcr._v2csv")
# Aggregate enriched 311 rat sightings to daily counts
#
# Reads output/rats_enriched.geojson, groups by date, and writes
# output/daily_rats.csv for the seasonality heat-map.
library(dplyr)
library(lubridate)
library(sf)
library(readr)
# 1) Read enriched points
rats <- sf::st_read("output/rats_enriched.geojson", quiet = TRUE) %>%
sf::st_drop_geometry()
# 2) Aggregate to one row per date
daily_counts <- rats %>%
mutate(date = as_date(ymd_hms(created_dt))) %>%
group_by(date) %>%
summarise(calls = n(), .groups = "drop")
# 3) Write CSV for Tableau
write_csv(daily_counts, "output/daily_rats.csv")
message("✅ Wrote daily counts to output/daily_rats.csv")
# Aggregate enriched 311 rat sightings to daily counts
#
# Reads output/rats_enriched.geojson, groups by date, and writes
# output/daily_rats.csv for the seasonality heat-map.
library(dplyr)
library(lubridate)
library(sf)
library(readr)
# 1) Read enriched points (with CD_ID)
rats <- sf::st_read("output/rats_enriched.geojson", quiet = TRUE) %>%
sf::st_drop_geometry()
# 2) Build daily counts by CD_ID
daily_cd <- rats %>%
mutate(date = as_date(ymd_hms(created_dt))) %>%
filter(!is.na(CD_ID)) %>%
group_by(CD_ID, date) %>%
summarise(calls = n(), .groups = "drop")
# 3) Write out
write_csv(daily_cd, "output/daily_rats_by_cd.csv")
message("✅ Wrote per-CD daily counts to output/daily_rats_by_cd.csv")
# Aggregate enriched 311 rat sightings to daily counts
#
# Reads output/rats_enriched.geojson, groups by date, and writes
# output/daily_rats.csv for the seasonality heat-map.
library(dplyr)
library(lubridate)
library(sf)
library(readr)
rats <- sf::st_read("output/rats_enriched.geojson", quiet=TRUE) %>%
st_drop_geometry() %>%
filter(!is.na(CD_ID)) %>%
mutate(quarter = floor_date(ymd_hms(created_dt), unit="quarter"))
quarterly_cd <- rats %>%
group_by(CD_ID, quarter) %>%
summarise(calls = n(), .groups="drop")
write_csv(quarterly_cd, "output/quarterly_rats_by_cd.csv")
message("✅ Wrote per-CD quarterly counts to output/quarterly_rats_by_cd.csv")
View(rats_ready)
